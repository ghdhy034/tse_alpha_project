#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç”Ÿç”¢ç´šç…™éœ§æ¸¬è©¦ - éšæ®µ3: å°è¦æ¨¡å¤šè‚¡ç¥¨æ¸¬è©¦
æ¸¬è©¦5æ”¯è‚¡ç¥¨çš„æ‰¹æ¬¡è™•ç†å’Œè¨˜æ†¶é«”ç®¡ç†
"""
import sys
import os
from pathlib import Path
import traceback
from datetime import datetime
import numpy as np
import torch
import psutil
import gc

# å¼·åˆ¶UTF-8è¼¸å‡º
sys.stdout.reconfigure(encoding='utf-8')

# æ·»åŠ è·¯å¾‘
sys.path.append(str(Path(__file__).parent))
sys.path.append(str(Path(__file__).parent / "models"))
sys.path.append(str(Path(__file__).parent / "data_pipeline"))
sys.path.append(str(Path(__file__).parent / "market_data_collector"))

def print_status(task, status, details=""):
    """çµ±ä¸€çš„ç‹€æ…‹è¼¸å‡ºæ ¼å¼"""
    timestamp = datetime.now().strftime("%H:%M:%S")
    status_icon = "âœ…" if status == "SUCCESS" else "âŒ" if status == "FAILED" else "ğŸ”„"
    print(f"[{timestamp}] {status_icon} {task}: {status}")
    if details:
        print(f"    è©³æƒ…: {details}")

def get_memory_usage():
    """ç²å–ç•¶å‰è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³"""
    process = psutil.Process()
    memory_info = process.memory_info()
    return {
        'rss_mb': memory_info.rss / 1024 / 1024,  # å¯¦éš›ä½¿ç”¨è¨˜æ†¶é«”
        'vms_mb': memory_info.vms / 1024 / 1024,  # è™›æ“¬è¨˜æ†¶é«”
        'percent': process.memory_percent()        # è¨˜æ†¶é«”ä½¿ç”¨ç™¾åˆ†æ¯”
    }

def task_3_1_multi_stock_feature_processing():
    """ä»»å‹™3.1: 5æ”¯è‚¡ç¥¨ç‰¹å¾µè™•ç†"""
    print("\n" + "="*60)
    print("ğŸ¯ ä»»å‹™3.1: 5æ”¯è‚¡ç¥¨ç‰¹å¾µè™•ç†")
    print("="*60)
    
    try:
        from data_pipeline.features import FeatureEngine
        
        # æ¸¬è©¦è‚¡ç¥¨æ¸…å–®
        test_symbols = ['2330', '2317', '2603', '2454', '2412']
        
        print(f"ğŸ“Š è™•ç†è‚¡ç¥¨æ¸…å–®: {test_symbols}")
        
        # è¨˜éŒ„åˆå§‹è¨˜æ†¶é«”
        initial_memory = get_memory_usage()
        print(f"ğŸ§  åˆå§‹è¨˜æ†¶é«”ä½¿ç”¨: {initial_memory['rss_mb']:.1f} MB ({initial_memory['percent']:.1f}%)")
        
        # å‰µå»ºç‰¹å¾µå¼•æ“
        print("âš™ï¸ åˆå§‹åŒ–ç‰¹å¾µå¼•æ“...")
        feature_engine = FeatureEngine(symbols=test_symbols)
        
        # æ‰¹æ¬¡è™•ç†å¤šè‚¡ç¥¨
        print("ğŸ”„ æ‰¹æ¬¡è™•ç†å¤šè‚¡ç¥¨ç‰¹å¾µ...")
        start_date = '2024-01-01'
        end_date = '2024-01-31'  # å°ç¯„åœæ¸¬è©¦
        
        results = feature_engine.process_multiple_symbols(
            symbols=test_symbols,
            start_date=start_date,
            end_date=end_date,
            normalize=True
        )
        
        # é©—è­‰çµæœ
        if not results:
            raise ValueError("æ²’æœ‰æˆåŠŸè™•ç†ä»»ä½•è‚¡ç¥¨")
        
        print(f"ğŸ“ˆ è™•ç†çµæœæ‘˜è¦:")
        total_features = 0
        total_records = 0
        
        for symbol, (features, labels, prices) in results.items():
            feature_count = features.shape[1] if not features.empty else 0
            record_count = features.shape[0] if not features.empty else 0
            
            print(f"   {symbol}: {feature_count}ç¶­ç‰¹å¾µ, {record_count}ç­†è¨˜éŒ„")
            
            total_features += feature_count
            total_records += record_count
            
            # æª¢æŸ¥ç‰¹å¾µå®Œæ•´æ€§
            if not features.empty:
                null_count = features.isnull().sum().sum()
                inf_count = np.isinf(features.select_dtypes(include=[np.number])).sum().sum()
                
                if null_count > record_count * 0.1:  # è¶…é10%ç©ºå€¼è­¦å‘Š
                    print(f"     âš ï¸ ç©ºå€¼è¼ƒå¤š: {null_count}")
                if inf_count > 0:
                    print(f"     âš ï¸ ç„¡é™å€¼: {inf_count}")
        
        # è¨˜éŒ„è™•ç†å¾Œè¨˜æ†¶é«”
        after_memory = get_memory_usage()
        memory_increase = after_memory['rss_mb'] - initial_memory['rss_mb']
        
        print(f"ğŸ§  è™•ç†å¾Œè¨˜æ†¶é«”: {after_memory['rss_mb']:.1f} MB (+{memory_increase:.1f} MB)")
        
        # é©—è­‰è™•ç†æˆåŠŸç‡
        success_rate = len(results) / len(test_symbols)
        if success_rate < 0.8:  # è‡³å°‘80%æˆåŠŸç‡
            raise ValueError(f"è™•ç†æˆåŠŸç‡éä½: {success_rate:.1%}")
        
        print_status("ä»»å‹™3.1", "SUCCESS", f"æˆåŠŸè™•ç†{len(results)}/{len(test_symbols)}æ”¯è‚¡ç¥¨ï¼Œç¸½è¨ˆ{total_records}ç­†è¨˜éŒ„")
        return True, results
        
    except Exception as e:
        print_status("ä»»å‹™3.1", "FAILED", str(e))
        traceback.print_exc()
        return False, None

def task_3_2_batch_data_loading_test(features_dict):
    """ä»»å‹™3.2: æ‰¹æ¬¡è³‡æ–™è¼‰å…¥æ¸¬è©¦"""
    print("\n" + "="*60)
    print("ğŸ¯ ä»»å‹™3.2: æ‰¹æ¬¡è³‡æ–™è¼‰å…¥æ¸¬è©¦")
    print("="*60)
    
    try:
        if not features_dict:
            raise ValueError("æ²’æœ‰å¯ç”¨çš„ç‰¹å¾µè³‡æ–™")
        
        from models.data_loader import TSEDataLoader, DataConfig
        
        # è¨˜éŒ„åˆå§‹è¨˜æ†¶é«”
        initial_memory = get_memory_usage()
        print(f"ğŸ§  åˆå§‹è¨˜æ†¶é«”: {initial_memory['rss_mb']:.1f} MB")
        
        # å‰µå»ºè³‡æ–™é…ç½®
        print("ğŸ“Š å‰µå»ºè³‡æ–™è¼‰å…¥é…ç½®...")
        symbols = list(features_dict.keys())
        
        data_config = DataConfig(
            symbols=symbols,
            train_start_date='2024-01-01',
            train_end_date='2024-01-20',
            val_start_date='2024-01-21',
            val_end_date='2024-01-25',
            test_start_date='2024-01-26',
            test_end_date='2024-01-31',
            sequence_length=32,  # è¼ƒçŸ­åºåˆ—ç”¨æ–¼æ¸¬è©¦
            batch_size=4,        # å°æ‰¹æ¬¡
            num_workers=0        # é¿å…å¤šé€²ç¨‹å•é¡Œ
        )
        
        print(f"   è‚¡ç¥¨æ•¸é‡: {len(symbols)}")
        print(f"   åºåˆ—é•·åº¦: {data_config.sequence_length}")
        print(f"   æ‰¹æ¬¡å¤§å°: {data_config.batch_size}")
        
        # å‰µå»ºè³‡æ–™è¼‰å…¥å™¨
        print("ğŸ”„ å‰µå»ºè³‡æ–™è¼‰å…¥å™¨...")
        data_loader = TSEDataLoader(data_config)
        
        # æ‰‹å‹•è¨­ç½®ç‰¹å¾µè³‡æ–™ (è·³éé‡æ–°è™•ç†)
        data_loader.features_dict = features_dict
        
        # ç²å–è³‡æ–™è¼‰å…¥å™¨
        print("ğŸ“¦ å‰µå»ºæ‰¹æ¬¡è¼‰å…¥å™¨...")
        train_loader, val_loader, test_loader = data_loader.get_dataloaders()
        
        print(f"   è¨“ç·´æ‰¹æ¬¡: {len(train_loader)}")
        print(f"   é©—è­‰æ‰¹æ¬¡: {len(val_loader)}")
        print(f"   æ¸¬è©¦æ‰¹æ¬¡: {len(test_loader)}")
        
        # æ¸¬è©¦æ‰¹æ¬¡è¼‰å…¥
        print("ğŸ§ª æ¸¬è©¦æ‰¹æ¬¡è¼‰å…¥...")
        batch_count = 0
        total_samples = 0
        
        for loader_name, loader in [("è¨“ç·´", train_loader), ("é©—è­‰", val_loader), ("æ¸¬è©¦", test_loader)]:
            if len(loader) == 0:
                print(f"   âš ï¸ {loader_name}è¼‰å…¥å™¨ç‚ºç©º")
                continue
            
            print(f"   æ¸¬è©¦{loader_name}è¼‰å…¥å™¨...")
            
            for i, batch in enumerate(loader):
                if i >= 2:  # åªæ¸¬è©¦å‰2å€‹æ‰¹æ¬¡
                    break
                
                # æª¢æŸ¥æ‰¹æ¬¡æ ¼å¼
                observation = batch['observation']
                labels = batch['labels']
                metadata = batch['metadata']
                
                batch_size = observation['price_frame'].shape[0]
                total_samples += batch_size
                batch_count += 1
                
                print(f"     æ‰¹æ¬¡{i+1}: {batch_size}å€‹æ¨£æœ¬")
                print(f"       price_frame: {observation['price_frame'].shape}")
                print(f"       fundamental: {observation['fundamental'].shape}")
                print(f"       account: {observation['account'].shape}")
                print(f"       labels: {labels.shape}")
                
                # æª¢æŸ¥è³‡æ–™æœ‰æ•ˆæ€§
                for key, tensor in observation.items():
                    if torch.isnan(tensor).any():
                        print(f"       âš ï¸ {key}åŒ…å«NaNå€¼")
                    if torch.isinf(tensor).any():
                        print(f"       âš ï¸ {key}åŒ…å«ç„¡é™å€¼")
        
        # è¨˜éŒ„è¼‰å…¥å¾Œè¨˜æ†¶é«”
        after_memory = get_memory_usage()
        memory_increase = after_memory['rss_mb'] - initial_memory['rss_mb']
        
        print(f"ğŸ§  è¼‰å…¥å¾Œè¨˜æ†¶é«”: {after_memory['rss_mb']:.1f} MB (+{memory_increase:.1f} MB)")
        
        if batch_count == 0:
            raise ValueError("æ²’æœ‰æˆåŠŸè¼‰å…¥ä»»ä½•æ‰¹æ¬¡")
        
        print_status("ä»»å‹™3.2", "SUCCESS", f"æˆåŠŸè¼‰å…¥{batch_count}å€‹æ‰¹æ¬¡ï¼Œç¸½è¨ˆ{total_samples}å€‹æ¨£æœ¬")
        return True, (train_loader, val_loader, test_loader)
        
    except Exception as e:
        print_status("ä»»å‹™3.2", "FAILED", str(e))
        traceback.print_exc()
        return False, None

def task_3_3_memory_monitoring():
    """ä»»å‹™3.3: è¨˜æ†¶é«”ä½¿ç”¨ç›£æ§"""
    print("\n" + "="*60)
    print("ğŸ¯ ä»»å‹™3.3: è¨˜æ†¶é«”ä½¿ç”¨ç›£æ§")
    print("="*60)
    
    try:
        from models.model_architecture import ModelConfig, TSEAlphaModel
        from models.config.training_config import TrainingConfig
        
        # è¨˜éŒ„åŸºæº–è¨˜æ†¶é«”
        baseline_memory = get_memory_usage()
        print(f"ğŸ§  åŸºæº–è¨˜æ†¶é«”: {baseline_memory['rss_mb']:.1f} MB")
        
        # è¼‰å…¥é…ç½®
        training_config = TrainingConfig()
        
        # æ¸¬è©¦æ¨¡å‹è¨˜æ†¶é«”ä½¿ç”¨
        print("ğŸ¤– æ¸¬è©¦æ¨¡å‹è¨˜æ†¶é«”ä½¿ç”¨...")
        model_config = ModelConfig(
            price_frame_shape=(5, 32, training_config.other_features),  # 5æ”¯è‚¡ç¥¨
            fundamental_dim=training_config.fundamental_features,
            account_dim=training_config.account_features
        )
        
        model = TSEAlphaModel(model_config)
        
        model_memory = get_memory_usage()
        model_increase = model_memory['rss_mb'] - baseline_memory['rss_mb']
        print(f"   æ¨¡å‹è¼‰å…¥å¾Œ: {model_memory['rss_mb']:.1f} MB (+{model_increase:.1f} MB)")
        
        # æ¸¬è©¦æ‰¹æ¬¡è™•ç†è¨˜æ†¶é«”
        print("ğŸ“¦ æ¸¬è©¦æ‰¹æ¬¡è™•ç†è¨˜æ†¶é«”...")
        batch_sizes = [1, 2, 4, 8]
        memory_usage = {}
        
        for batch_size in batch_sizes:
            # å‰µå»ºæ¸¬è©¦æ‰¹æ¬¡
            observation = {
                'price_frame': torch.randn(batch_size, 5, 32, training_config.other_features),
                'fundamental': torch.randn(batch_size, training_config.fundamental_features),
                'account': torch.randn(batch_size, training_config.account_features)
            }
            
            # å‰å‘å‚³æ’­
            with torch.no_grad():
                outputs = model(observation)
            
            # è¨˜éŒ„è¨˜æ†¶é«”
            batch_memory = get_memory_usage()
            memory_usage[batch_size] = batch_memory['rss_mb']
            
            print(f"   æ‰¹æ¬¡å¤§å°{batch_size}: {batch_memory['rss_mb']:.1f} MB")
            
            # æ¸…ç†
            del observation, outputs
            torch.cuda.empty_cache() if torch.cuda.is_available() else None
            gc.collect()
        
        # æ¸¬è©¦è¨˜æ†¶é«”æ´©æ¼
        print("ğŸ” æ¸¬è©¦è¨˜æ†¶é«”æ´©æ¼...")
        initial_test_memory = get_memory_usage()
        
        # åŸ·è¡Œå¤šæ¬¡å‰å‘å‚³æ’­
        for i in range(10):
            observation = {
                'price_frame': torch.randn(2, 5, 32, training_config.other_features),
                'fundamental': torch.randn(2, training_config.fundamental_features),
                'account': torch.randn(2, training_config.account_features)
            }
            
            with torch.no_grad():
                outputs = model(observation)
            
            # ç«‹å³æ¸…ç†
            del observation, outputs
            
            if i % 3 == 0:  # æ¯3æ¬¡å¼·åˆ¶åƒåœ¾å›æ”¶
                gc.collect()
        
        final_test_memory = get_memory_usage()
        leak_amount = final_test_memory['rss_mb'] - initial_test_memory['rss_mb']
        
        print(f"   æ¸¬è©¦å‰: {initial_test_memory['rss_mb']:.1f} MB")
        print(f"   æ¸¬è©¦å¾Œ: {final_test_memory['rss_mb']:.1f} MB")
        print(f"   è¨˜æ†¶é«”è®ŠåŒ–: {leak_amount:+.1f} MB")
        
        # æª¢æŸ¥è¨˜æ†¶é«”æ´©æ¼
        if leak_amount > 50:  # è¶…é50MBèªç‚ºæœ‰æ´©æ¼
            print(f"   âš ï¸ å¯èƒ½å­˜åœ¨è¨˜æ†¶é«”æ´©æ¼: {leak_amount:.1f} MB")
        
        # åˆ†æè¨˜æ†¶é«”ä½¿ç”¨æ¨¡å¼
        print("ğŸ“Š è¨˜æ†¶é«”ä½¿ç”¨åˆ†æ:")
        max_memory = max(memory_usage.values())
        min_memory = min(memory_usage.values())
        memory_range = max_memory - min_memory
        
        print(f"   æœ€å¤§ä½¿ç”¨: {max_memory:.1f} MB")
        print(f"   æœ€å°ä½¿ç”¨: {min_memory:.1f} MB")
        print(f"   ä½¿ç”¨ç¯„åœ: {memory_range:.1f} MB")
        
        # æª¢æŸ¥è¨˜æ†¶é«”æ•ˆç‡
        if memory_range > 200:  # è¨˜æ†¶é«”ä½¿ç”¨è®ŠåŒ–éå¤§
            print(f"   âš ï¸ è¨˜æ†¶é«”ä½¿ç”¨è®ŠåŒ–è¼ƒå¤§: {memory_range:.1f} MB")
        
        print_status("ä»»å‹™3.3", "SUCCESS", f"è¨˜æ†¶é«”ç›£æ§å®Œæˆï¼Œæ´©æ¼æª¢æ¸¬: {leak_amount:+.1f} MB")
        return True
        
    except Exception as e:
        print_status("ä»»å‹™3.3", "FAILED", str(e))
        traceback.print_exc()
        return False

def run_stage3_multi_stock_test():
    """åŸ·è¡Œéšæ®µ3: å°è¦æ¨¡å¤šè‚¡ç¥¨æ¸¬è©¦"""
    print("ğŸš€ é–‹å§‹éšæ®µ3: å°è¦æ¨¡å¤šè‚¡ç¥¨æ¸¬è©¦")
    print("="*80)
    
    start_time = datetime.now()
    
    # åŸ·è¡Œä»»å‹™3.1
    success_3_1, features_dict = task_3_1_multi_stock_feature_processing()
    
    # åŸ·è¡Œä»»å‹™3.2
    success_3_2, data_loaders = task_3_2_batch_data_loading_test(features_dict) if success_3_1 else (False, None)
    
    # åŸ·è¡Œä»»å‹™3.3
    success_3_3 = task_3_3_memory_monitoring()
    
    # ç¸½çµ
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()
    
    results = {
        "ä»»å‹™3.1": success_3_1,
        "ä»»å‹™3.2": success_3_2,
        "ä»»å‹™3.3": success_3_3
    }
    
    print("\n" + "="*80)
    print("ğŸ“‹ éšæ®µ3åŸ·è¡Œç¸½çµ")
    print("="*80)
    
    success_count = sum(1 for success in results.values() if success)
    total_count = len(results)
    
    for task_name, success in results.items():
        status = "âœ… æˆåŠŸ" if success else "âŒ å¤±æ•—"
        print(f"   {task_name}: {status}")
    
    print(f"\nğŸ“Š ç¸½é«”çµæœ: {success_count}/{total_count} ä»»å‹™æˆåŠŸ")
    print(f"â±ï¸ åŸ·è¡Œæ™‚é–“: {duration:.1f} ç§’")
    
    # æœ€çµ‚è¨˜æ†¶é«”æª¢æŸ¥
    final_memory = get_memory_usage()
    print(f"ğŸ§  æœ€çµ‚è¨˜æ†¶é«”ä½¿ç”¨: {final_memory['rss_mb']:.1f} MB ({final_memory['percent']:.1f}%)")
    
    if success_count == total_count:
        print("ğŸ‰ éšæ®µ3: å°è¦æ¨¡å¤šè‚¡ç¥¨æ¸¬è©¦ - å…¨éƒ¨é€šéï¼")
        print("âœ… æº–å‚™é€²å…¥éšæ®µ4: è¨“ç·´æµç¨‹é©—è­‰")
        return True
    else:
        print("âš ï¸ éšæ®µ3: å°è¦æ¨¡å¤šè‚¡ç¥¨æ¸¬è©¦ - éƒ¨åˆ†å¤±æ•—")
        print("âŒ éœ€è¦ä¿®å¾©å•é¡Œå¾Œå†ç¹¼çºŒ")
        return False

if __name__ == "__main__":
    try:
        success = run_stage3_multi_stock_test()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ¶ä¸­æ–·æ¸¬è©¦")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ æœªé æœŸçš„éŒ¯èª¤: {e}")
        traceback.print_exc()
        sys.exit(1)
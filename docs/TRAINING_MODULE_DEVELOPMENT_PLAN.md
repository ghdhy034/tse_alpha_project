# TSE Alpha è¨“ç·´æ¨¡çµ„é–‹ç™¼è¦åŠƒ

> **åŸºæº–æ–‡æª”**: `training_module_ssot.md` + `References.txt`  
> **å‰µå»ºæ—¥æœŸ**: 2025-01-15  
> **æ›´æ–°æ—¥æœŸ**: 2025-01-15 (åŸºæ–¼ References.txt å„ªåŒ–)  
> **é–‹ç™¼æ¨¡å¼**: ğŸ‘¥ ä¸€äººåœ˜éšŠ + AI å”ä½œ  
> **ç‹€æ…‹**: ğŸš€ æº–å‚™é–‹ç™¼  

## ğŸ“‹ **é–‹ç™¼åŸºæº–èˆ‡ç´„æŸ**

### **SSOT è¦ç¯„éµå¾ª**
- **è‚¡ç¥¨åˆ†å‰²**: 126 è¨“ç·´ + 27 é©—è­‰ + 27 æ¸¬è©¦ (åŸºæ–¼ `stock_split_config.json`)
- **ç‰¹å¾µç¶­åº¦**: 75å€‹ (18åŸºæœ¬é¢ + 53å…¶ä»– + 4å¸³æˆ¶)
- **åºåˆ—é…ç½®**: seq_len=64, stride=32, forward_window=15
- **è³‡æ–™ä¾†æº**: 8å€‹æ ¸å¿ƒè³‡æ–™è¡¨ (åŸºæ–¼ `db_structure.json`)

### **å„ªåŒ–å¾ŒæŠ€è¡“æ¶æ§‹** (åŸºæ–¼ References.txt)
```
æŠ€è¡“é¸å‹å„ªåŒ–:
â”œâ”€â”€ é…ç½®ç®¡ç†: Hydra + OmegaConf âœ… (æ›¿ä»£è‡ªå¯« ConfigManager)
â”œâ”€â”€ è¨“ç·´æ¡†æ¶: PyTorch Lightning âœ… (æ›¿ä»£è‡ªå¯« Trainer/Checkpoint)
â”œâ”€â”€ è³‡æ–™è™•ç†: PyArrow + Memory Mapped âœ… (å„ªåŒ– I/O æ€§èƒ½)
â”œâ”€â”€ å¯¦é©—è¿½è¹¤: Lightning Logger âœ… (TensorBoard + CSV + JSON)
â”œâ”€â”€ RL æ•´åˆ: Stable-Baselines3 âœ… (PPO baseline policy)
â”œâ”€â”€ è¶…åƒæ•¸å„ªåŒ–: Optuna + Hydra Sweep âœ… (å…§å»ºæ•´åˆ)
â””â”€â”€ ç‰¹å¾µç®¡ç†: Singleton Registry âœ… (core å¥—ä»¶çµ±ä¸€)
```

### **é›™ç¡¬é«”ç’°å¢ƒé…ç½®ç­–ç•¥**

#### **é–‹ç™¼/æ¸¬è©¦ç’°å¢ƒ** (GTX 1660 Ti 6GB)
```
ä½é…ç½®ç­–ç•¥ (ç…™éœ§æ¸¬è©¦/åˆæ­¥é©—è­‰):
â”œâ”€â”€ æ‰¹æ¬¡å¤§å°: batch=8 (ä¿å®ˆ VRAM ä½¿ç”¨)
â”œâ”€â”€ åºåˆ—é•·åº¦: seq_len=32 (æ¸›åŠ)
â”œâ”€â”€ è‚¡ç¥¨æ•¸é‡: 10æª” (å¿«é€Ÿé©—è­‰)
â”œâ”€â”€ è¨“ç·´è¼ªæ•¸: epoch=2-5 (ç…™éœ§æ¸¬è©¦)
â”œâ”€â”€ æ··åˆç²¾åº¦: precision=16 (ç¯€çœ VRAM)
â”œâ”€â”€ æ¢¯åº¦ç´¯ç©: accumulate_grad_batches=8 (ç­‰æ•ˆ batch=64)
â””â”€â”€ è³‡æ–™å­é›†: 20% è³‡æ–™ (å¿«é€Ÿè¿­ä»£)
```

#### **ç”Ÿç”¢/è¨“ç·´ç’°å¢ƒ** (RTX 4090 24GB)
```
é«˜é…ç½®ç­–ç•¥ (å®Œæ•´è¨“ç·´):
â”œâ”€â”€ æ‰¹æ¬¡å¤§å°: batch=128 (å……åˆ†åˆ©ç”¨ VRAM)
â”œâ”€â”€ åºåˆ—é•·åº¦: seq_len=64 (å®Œæ•´åºåˆ—)
â”œâ”€â”€ è‚¡ç¥¨æ•¸é‡: 180æª” (å®Œæ•´è‚¡ç¥¨æ± )
â”œâ”€â”€ è¨“ç·´è¼ªæ•¸: epoch=150-300 (å……åˆ†è¨“ç·´)
â”œâ”€â”€ æ··åˆç²¾åº¦: precision=16 (æ€§èƒ½å„ªåŒ–)
â”œâ”€â”€ æ¢¯åº¦ç´¯ç©: accumulate_grad_batches=1 (ç„¡éœ€ç´¯ç©)
â”œâ”€â”€ å®Œæ•´è³‡æ–™é›†: 100% è³‡æ–™ (1,200è¬+ç­†)
â””â”€â”€ è¶…åƒæ•¸æœç´¢: å¤§è¦æ¨¡ Optuna trials
```

### **ç¡¬é«”ç´„æŸè€ƒé‡** (åŸºæ–¼ References.txt)
- **GPU**: GTX 1660 Ti (6GB VRAM) - éœ€è¦è¬¹æ…çš„è¨˜æ†¶é«”ç®¡ç†
- **æ‰¹æ¬¡å¤§å°**: å»ºè­° batch=16, seq_len=64 (é¿å… OOM)
- **æ¢¯åº¦ç´¯ç©**: å•Ÿç”¨ gradient accumulation æ¨¡æ“¬å¤§æ‰¹æ¬¡
- **è³‡æ–™è¼‰å…¥**: ä½¿ç”¨ Memory Mapped File + prefetch å„ªåŒ–

---

## ğŸ¯ **é–‹ç™¼éšæ®µè¦åŠƒ** (å„ªåŒ–ç‰ˆ)

> **å”ä½œæ¨¡å¼**: ä¸€äººåœ˜éšŠ + AI åŠ©æ‰‹ï¼Œé«˜å½ˆæ€§æ™‚ç¨‹èª¿æ•´  
> **æŠ€è¡“ç­–ç•¥**: æ¡ç”¨æˆç†Ÿæ¡†æ¶æ¸›å°‘é‡è¤‡é€ è¼ªå­ (åŸºæ–¼ References.txt)

## **éšæ®µ 1: æ ¸å¿ƒé©—è­‰èˆ‡GPUè³‡æºç¢ºèª** (å½ˆæ€§ 3-5 å¤©)

### **1.1 GPU è³‡æºé©—è­‰ (å„ªå…ˆç´šæœ€é«˜)**
```python
# å‰µå»º: scripts/gpu_memory_test.py
class GPUResourceValidator:
    """GTX 1660 Ti (6GB) è³‡æºé©—è­‰"""
    
    def test_dataloader_memory(self):
        """DataLoader è¨˜æ†¶é«”å³°å€¼æ¸¬è©¦"""
        - batch_size=16, seq_len=64 æ¸¬è©¦
        - ç›£æ§ VRAM ä½¿ç”¨å³°å€¼
        - ç¢ºå®šæœ€ä½³ num_workers è¨­å®š
    
    def test_model_memory(self):
        """æ¨¡å‹è¨˜æ†¶é«”éœ€æ±‚æ¸¬è©¦"""
        - Conv1D + Transformer å‰å‘å‚³æ’­
        - æ¢¯åº¦è¨ˆç®—è¨˜æ†¶é«”éœ€æ±‚
        - ç¢ºå®šæ˜¯å¦éœ€è¦ gradient accumulation
    
    def optimize_batch_config(self):
        """å„ªåŒ–æ‰¹æ¬¡é…ç½®"""
        - æ‰¾å‡ºæœ€å¤§å¯ç”¨ batch_size
        - è¨­å®š gradient accumulation steps
        - é…ç½® mixed precision training
```

### **1.2 SSOT ç›¸å®¹æ€§é©—è­‰å™¨**
```python
# å‰µå»º: scripts/validate_ssot_compliance.py
class SSOTValidator:
    """é©—è­‰ç¾æœ‰å¯¦ä½œèˆ‡ SSOT è¦ç¯„çš„ç›¸å®¹æ€§"""
    
    def validate_feature_dimensions(self):
        """é©—è­‰ç‰¹å¾µç¶­åº¦ (ä¿®æ­£åŸºæ–¼ References.txt)"""
        - åºåˆ—ç‰¹å¾µ: 75å€‹ (18åŸºæœ¬é¢ + 53å…¶ä»– + 4å¸³æˆ¶)
        - å¸³æˆ¶ç‰¹å¾µ: 4å€‹ (ä½œç‚º env info å–®ç¨æ³¨å…¥)
        - æ˜ç¢ºå€åˆ† sequence features vs. portfolio features
    
    def validate_stock_splits(self):
        """é©—è­‰è‚¡ç¥¨åˆ†å‰²é…ç½®"""
        - 126/27/27 åˆ†å‰²æ¯”ä¾‹æª¢æŸ¥
        - ç„¡é‡è¤‡è‚¡ç¥¨é©—è­‰
        - è³‡æ–™åº«å®Œæ•´æ€§æª¢æŸ¥
```

**äº¤ä»˜ç‰©**:
- [ ] `scripts/gpu_memory_test.py` â­ (æœ€é«˜å„ªå…ˆç´š)
- [ ] GPU è³‡æºä½¿ç”¨å ±å‘Š
- [ ] å„ªåŒ–çš„æ‰¹æ¬¡é…ç½®å»ºè­°
- [ ] `scripts/validate_ssot_compliance.py`

### **1.3 ç‰¹å¾µè¨»å†Šç³»çµ± (Singleton æ¨¡å¼)**
```python
# å‰µå»º: core/features_registry.py (Singleton æ¨¡å¼)
class FeaturesRegistry:
    """çµ±ä¸€ç‰¹å¾µå®šç¾© (Singletonï¼Œé¿å…é‡è¤‡å®šç¾©)"""
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    # åºåˆ—ç‰¹å¾µ (75å€‹) - æ˜ç¢ºåˆ†é›¢
    SEQUENCE_FEATURES = {
        'price_volume': ['open', 'high', 'low', 'close', 'volume'],  # 5å€‹
        'technical': [  # 22å€‹æŠ€è¡“æŒ‡æ¨™
            'sma_5', 'sma_20', 'sma_60', 'ema_12', 'ema_26', 'ema_50', 
            'macd', 'macd_signal', 'macd_hist', 'rsi_14', 'stoch_k', 'stoch_d',
            'atr', 'adx', 'cci', 'obv', 'keltner_upper', 'keltner_middle', 
            'keltner_lower', 'bollinger_upper', 'bollinger_middle', 'bollinger_lower'
        ],
        'fundamental': [  # 43å€‹åŸºæœ¬é¢ç‰¹å¾µ
            # è²¡å‹™æŒ‡æ¨™ (20å€‹)
            'revenue', 'cost_of_goods_sold', 'gross_profit', 'operating_income', 'net_income',
            'total_assets', 'current_assets', 'total_liabilities', 'current_liabilities', 'equity',
            'cash_and_equivalents', 'inventory', 'accounts_receivable', 'accounts_payable', 'debt',
            'eps', 'book_value_per_share', 'dividend_per_share', 'roe', 'roa',
            # è²¡å‹™æ¯”ç‡ (15å€‹)
            'pe_ratio', 'pb_ratio', 'ps_ratio', 'debt_to_equity', 'current_ratio',
            'quick_ratio', 'gross_margin', 'operating_margin', 'net_margin', 'asset_turnover',
            'inventory_turnover', 'receivables_turnover', 'dividend_yield', 'payout_ratio', 'interest_coverage',
            # æˆé•·ç‡ (8å€‹)
            'revenue_growth_yoy', 'eps_growth_yoy', 'asset_growth_yoy', 'equity_growth_yoy',
            'revenue_growth_qoq', 'eps_growth_qoq', 'asset_growth_qoq', 'equity_growth_qoq'
        ]
    }
    
    # å¸³æˆ¶ç‰¹å¾µ (4å€‹) - ä½œç‚º env info å–®ç¨æ³¨å…¥
    PORTFOLIO_FEATURES = ['nav_change', 'position_ratio', 'unrealized_pnl', 'risk_buffer']
    
    @property
    def total_sequence_features(self):
        """ç¸½åºåˆ—ç‰¹å¾µæ•¸: 75å€‹"""
        return len(self.SEQUENCE_FEATURES['price_volume']) + \
               len(self.SEQUENCE_FEATURES['technical']) + \
               len(self.SEQUENCE_FEATURES['fundamental'])
```

**äº¤ä»˜ç‰©**:
- [ ] `core/features_registry.py` (Singleton æ¨¡å¼)
- [ ] åºåˆ—ç‰¹å¾µ vs å¸³æˆ¶ç‰¹å¾µåˆ†é›¢æ–‡æª”
- [ ] ç‰¹å¾µç¶­åº¦é©—è­‰æ¸¬è©¦

### **1.3 æ¡ç”¨æˆç†Ÿæ¡†æ¶ (åŸºæ–¼ References.txt å»ºè­°)**
```python
# é…ç½®ç®¡ç†: æ¡ç”¨ omegaconf + hydra-core
# å®‰è£: pip install omegaconf hydra-core

# å‰µå»º: configs/training_config.yaml
defaults:
  - _self_
  - model: conv1d_transformer
  - data: tse_alpha

# æ¨¡å‹è¨“ç·´: æ¡ç”¨ PyTorch Lightning
# å®‰è£: pip install pytorch-lightning

class TSEAlphaLightningModule(pl.LightningModule):
    """åŸºæ–¼ Lightning çš„è¨“ç·´æ¨¡çµ„"""
    
    def __init__(self, config):
        super().__init__()
        self.model = TSEAlphaModel(config.model)
        self.config = config
    
    def training_step(self, batch, batch_idx):
        """è‡ªå‹•è™•ç† GPUã€æ¢¯åº¦ç´¯ç©ã€æª¢æŸ¥é»"""
        # Lightning è‡ªå‹•è™•ç†è¤‡é›œçš„è¨“ç·´é‚è¼¯
    
    def configure_optimizers(self):
        """å„ªåŒ–å™¨é…ç½®"""
        # æ”¯æ´å­¸ç¿’ç‡èª¿åº¦ã€å¤šå„ªåŒ–å™¨ç­‰
```

**äº¤ä»˜ç‰©**:
- [ ] Hydra é…ç½®ç³»çµ±è¨­ç½®
- [ ] Lightning æ¨¡çµ„é‡æ§‹
- [ ] è¨˜æ†¶é«”å„ªåŒ–çš„ DataLoader

---

## **éšæ®µ 2: è¼•é‡åŒ–è¨“ç·´ç®¡ç·š** (å½ˆæ€§ 4-6 å¤©)

### **2.1 Lightning è¨“ç·´ç®¡ç·š (ç°¡åŒ–ç‰ˆ)**
```python
# å‰µå»º: training/lightning_trainer.py
@hydra.main(config_path="configs", config_name="training_config")
def train(cfg: DictConfig):
    """Hydra + Lightning ç°¡åŒ–è¨“ç·´ç®¡ç·š"""
    
    # è³‡æ–™æ¨¡çµ„ (Lightning DataModule)
    data_module = TSEDataModule(cfg.data)
    
    # æ¨¡å‹æ¨¡çµ„ (Lightning Module)  
    model = TSEAlphaLightningModule(cfg)
    
    # è¨“ç·´å™¨ (Lightning Trainer)
    trainer = pl.Trainer(
        max_epochs=cfg.training.num_epochs,
        gpus=1 if torch.cuda.is_available() else 0,
        precision=16,  # æ··åˆç²¾åº¦ç¯€çœè¨˜æ†¶é«”
        gradient_clip_val=cfg.training.gradient_clip_norm,
        accumulate_grad_batches=cfg.training.accumulate_grad_batches,  # æ¨¡æ“¬å¤§æ‰¹æ¬¡
        callbacks=[
            EarlyStopping(patience=cfg.training.early_stopping_patience),
            ModelCheckpoint(save_top_k=3),
            LearningRateMonitor()
        ]
    )
    
    # åŸ·è¡Œè¨“ç·´ (Lightning è‡ªå‹•è™•ç†æ‰€æœ‰è¤‡é›œé‚è¼¯)
    trainer.fit(model, data_module)

# RL Agent æ•´åˆ (åŸºæ–¼ References.txt å»ºè­°)
class TSEAlphaRLAgent:
    """RL Agent èˆ‡ç’°å¢ƒä»‹æ¥"""
    
    def __init__(self, model, env):
        self.model = model
        self.env = env
        
    def train_rl(self):
        """RL è¨“ç·´å†’ç…™æ¸¬è©¦"""
        # ç¢ºä¿ Week 3 æ··åˆè¨“ç·´å°±ç·’
```

**äº¤ä»˜ç‰©**:
- [ ] Lightning è¨“ç·´ç®¡ç·š
- [ ] Hydra é…ç½®æ–‡ä»¶
- [ ] RL Agent å†’ç…™æ¸¬è©¦ â­ (ç‚º Week 3 æº–å‚™)

### **2.2 è¨˜æ†¶é«”å„ªåŒ–è³‡æ–™è¼‰å…¥**
```python
# å„ªåŒ–: models/data_loader.py (åŸºæ–¼ References.txt)
class TSEDataModuleOptimized(pl.LightningDataModule):
    """è¨˜æ†¶é«”å„ªåŒ–çš„ Lightning DataModule"""
    
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.batch_size = 16  # GTX 1660 Ti å®‰å…¨æ‰¹æ¬¡å¤§å°
    
    def setup(self, stage=None):
        """ä½¿ç”¨ Memory Mapped File + Arrow"""
        # pyarrow.dataset + Memory Mapped File
        self.train_dataset = ArrowDataset("data/train.arrow")
        self.val_dataset = ArrowDataset("data/val.arrow")
    
    def train_dataloader(self):
        return DataLoader(
            self.train_dataset,
            batch_size=self.batch_size,
            num_workers=2,  # åŸºæ–¼ GPU æ¸¬è©¦çµæœèª¿æ•´
            prefetch_factor=2,  # é å–å„ªåŒ–
            pin_memory=True
        )
```

**äº¤ä»˜ç‰©**:
- [ ] è¨˜æ†¶é«”å„ªåŒ–çš„ DataModule
- [ ] Arrow è³‡æ–™æ ¼å¼è½‰æ›
- [ ] æ‰¹æ¬¡å¤§å°è‡ªå‹•èª¿æ•´

### **2.3 Lightning Callbacks (å–ä»£è‡ªå¯«æª¢æŸ¥é»)**
```python
# å‰µå»º: training/callbacks.py
class TSEAlphaCallbacks:
    """Lightning Callbacks (çœå»è‡ªå¯«æª¢æŸ¥é»ç³»çµ±)"""
    
    @staticmethod
    def get_callbacks(cfg: DictConfig):
        """ç²å– Lightning Callbacks"""
        from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor
        
        return [
            ModelCheckpoint(
                monitor='val_loss',
                mode='min',
                save_top_k=3,
                filename='tse-alpha-{epoch:02d}-{val_loss:.2f}'
            ),
            EarlyStopping(
                monitor='val_loss',
                patience=cfg.training.early_stopping_patience,
                mode='min'
            ),
            LearningRateMonitor(logging_interval='epoch'),
            # è‡ªå®šç¾© RL å¾®èª¿ Callback
            RLFinetuningCallback(cfg.rl) if cfg.training.mode == 'hybrid' else None
        ]

class RLFinetuningCallback(pl.Callback):
    """RL å¾®èª¿ Callback (æ”¯æ´æ··åˆè¨“ç·´)"""
    
    def on_train_epoch_end(self, trainer, pl_module):
        """ç›£ç£å­¸ç¿’çµæŸå¾Œåˆ‡æ›åˆ° RL"""
        if trainer.current_epoch == pl_module.cfg.training.supervised_epochs:
            # åˆ‡æ›åˆ° RL æ¨¡å¼
            self.switch_to_rl_mode(trainer, pl_module)
```

**äº¤ä»˜ç‰©**:
- [ ] `training/callbacks.py` (Lightning Callbacks)
- [ ] RL å¾®èª¿ Callback å¯¦ä½œ
- [ ] Lightning Logger é…ç½® (TensorBoard + CSV)

---

## **éšæ®µ 3: æ™ºèƒ½è¶…åƒæ•¸å„ªåŒ–** (å½ˆæ€§ 3-4 å¤©)

### **3.1 Optuna æ•´åˆç³»çµ±**
```python
# å‰µå»º: training/optuna_optimizer.py
class OptunaOptimizer:
    """åŸºæ–¼ SSOT çš„ Optuna è¶…åƒæ•¸å„ªåŒ–"""
    
    def create_study(self):
        """å‰µå»º Optuna ç ”ç©¶"""
        - å®šç¾©æœç´¢ç©ºé–“ (åŸºæ–¼ SSOT)
        - é…ç½® Pruner
        - è¨­å®šå­˜å„²å¾Œç«¯
    
    def define_search_space(self, trial):
        """å®šç¾©æœç´¢ç©ºé–“ (åŸºæ–¼ References.txt å„ªåŒ–)"""
        # éšæ¢¯å‹æœç´¢é¿å… VRAM çˆ†ç‚¸
        d_model = trial.suggest_categorical('d_model', [256, 384, 512])  # é™ä½ä¸Šé™
        seq_len = trial.suggest_categorical('seq_len', [32, 48, 64])     # å‹•æ…‹åºåˆ—é•·åº¦
        
        # æ ¹æ“šæ¨¡å‹å¤§å°èª¿æ•´æ‰¹æ¬¡å¤§å°
        if d_model >= 512 or seq_len >= 64:
            batch_size = 8
        else:
            batch_size = 16
            
        return {
            'lr': trial.suggest_loguniform('lr', 1e-4, 8e-4),
            'd_model': d_model,
            'seq_len': seq_len,
            'batch_size': batch_size,
            'n_layer': trial.suggest_int('n_layer', 3, 6),  # é™ä½ä¸Šé™
            'dropout': trial.suggest_uniform('dropout', 0.1, 0.4),
            'alpha_beta': trial.suggest_uniform('alpha_beta', 0.3, 0.7)
        }
    
    def objective(self, trial):
        """Optuna ç›®æ¨™å‡½æ•¸ (åŸºæ–¼ References.txt å„ªåŒ–)"""
        params = self.define_search_space(trial)
        
        # Mini-epoch è©•ä¼° (20% è³‡æ–™) ç¯€çœæ™‚é–“
        trainer = pl.Trainer(
            max_epochs=5,  # å¿«é€Ÿè©•ä¼°
            limit_train_batches=0.2,  # åªç”¨ 20% è³‡æ–™
            limit_val_batches=0.5,    # é©—è­‰ç”¨ 50% è³‡æ–™
            enable_checkpointing=False,  # ç¯€çœ I/O
            logger=False  # æ¸›å°‘æ—¥èªŒé–‹éŠ·
        )
        
        # è¿”å› proxy metric
        return trainer.callback_metrics['val_loss'].item()
```

**äº¤ä»˜ç‰©**:
- [ ] `training/optuna_optimizer.py`
- [ ] Optuna é…ç½®æ¨¡æ¿
- [ ] è¶…åƒæ•¸å„ªåŒ–è…³æœ¬

### **3.2 Arrow åˆ†ç‰‡ + Memory Mapped I/O**
```python
# å‰µå»º: data/arrow_dataset.py
class ArrowDataset:
    """Arrow åˆ†ç‰‡è³‡æ–™é›† (è§£æ±º I/O ç“¶é ¸)"""
    
    def __init__(self, arrow_path: str):
        import pyarrow.dataset as ds
        
        # Memory Mapped File è¼‰å…¥
        self.dataset = ds.dataset(arrow_path, format='arrow')
        self.table = self.dataset.to_table()
    
    def __getitem__(self, idx):
        """Memory Mapped å­˜å–"""
        # ç›´æ¥å¾è¨˜æ†¶é«”æ˜ å°„è®€å–ï¼Œé¿å…é‡è¤‡è¼‰å…¥
        return self.table.slice(idx, 1).to_pandas()
    
    def create_shards(self, train_stocks, val_stocks, test_stocks):
        """å‰µå»º Arrow åˆ†ç‰‡"""
        # æŒ‰è‚¡ç¥¨åˆ†å‰²å‰µå»ºåˆ†ç‰‡
        train_table = self.filter_by_stocks(train_stocks)
        val_table = self.filter_by_stocks(val_stocks)
        test_table = self.filter_by_stocks(test_stocks)
        
        # ä¿å­˜åˆ†ç‰‡
        train_table.write('data/train.arrow')
        val_table.write('data/validation.arrow')
        test_table.write('data/test.arrow')
```

**äº¤ä»˜ç‰©**:
- [ ] `training/experiment_manager.py`
- [ ] å¯¦é©—è¿½è¹¤å„€è¡¨æ¿
- [ ] çµæœæ¯”è¼ƒå·¥å…·

---

## **éšæ®µ 4: æ ¸å¿ƒè·¯å¾‘æ¸¬è©¦** (å½ˆæ€§ 2-3 å¤©)

### **4.1 æ ¸å¿ƒè·¯å¾‘ Smoke Tests (åŸºæ–¼ References.txt)**
```python
# å‰µå»º: tests/core_tests/ (èšç„¦æ ¸å¿ƒè·¯å¾‘)
â”œâ”€â”€ test_ssot_validation.py      # SSOT é©—è­‰æ ¸å¿ƒè·¯å¾‘
â”œâ”€â”€ test_dataloader_memory.py    # DataLoader è¨˜æ†¶é«”å®‰å…¨
â”œâ”€â”€ test_trainer_loop.py         # Trainer Loop åŸºæœ¬åŠŸèƒ½
â”œâ”€â”€ test_env_compatibility.py    # Env ç›¸å®¹æ€§æ ¸å¿ƒè·¯å¾‘
â””â”€â”€ test_gpu_utilization.py      # GPU è³‡æºä½¿ç”¨é©—è­‰

def test_dataloader_memory():
    """DataLoader è¨˜æ†¶é«”å®‰å…¨æ¸¬è©¦"""
    # GTX 1660 Ti ç‰¹å®šæ¸¬è©¦
    dataloader = create_dataloader(batch_size=16, seq_len=64)
    
    initial_memory = torch.cuda.memory_allocated()
    for batch in dataloader:
        current_memory = torch.cuda.memory_allocated()
        assert current_memory < 5.5 * 1024**3  # < 5.5GB å®‰å…¨ç·š
        break
        
def test_trainer_loop():
    """Lightning Trainer æ ¸å¿ƒåŠŸèƒ½"""
    trainer = pl.Trainer(fast_dev_run=True)  # å¿«é€Ÿæ¸¬è©¦æ¨¡å¼
    result = trainer.fit(model, datamodule)
    assert result  # ç„¡ä¾‹å¤–å®Œæˆ
```

**äº¤ä»˜ç‰©**:
- [ ] æ ¸å¿ƒè·¯å¾‘æ¸¬è©¦å¥—ä»¶
- [ ] GPU è¨˜æ†¶é«”ç›£æ§
- [ ] Type Hint éœæ…‹æª¢æŸ¥

### **4.2 æ•´åˆæ¸¬è©¦**
```python
# å‰µå»º: tests/integration/
â”œâ”€â”€ test_full_pipeline.py        # ç«¯åˆ°ç«¯æµç¨‹æ¸¬è©¦
â”œâ”€â”€ test_model_env_compat.py     # æ¨¡å‹èˆ‡ç’°å¢ƒç›¸å®¹æ€§
â”œâ”€â”€ test_optuna_integration.py   # è¶…åƒæ•¸å„ªåŒ–æ¸¬è©¦
â”œâ”€â”€ test_checkpoint_recovery.py  # æª¢æŸ¥é»æ¢å¾©æ¸¬è©¦
â””â”€â”€ test_multi_gpu_training.py   # å¤šGPUè¨“ç·´æ¸¬è©¦

def test_full_pipeline():
    """å®Œæ•´ç®¡ç·šæ¸¬è©¦"""
    - è³‡æ–™è¼‰å…¥ â†’ æ¨¡å‹è¨“ç·´ â†’ ç’°å¢ƒè©•ä¼°
    - æª¢æŸ¥é»ä¿å­˜ â†’ æ¨¡å‹è¼‰å…¥ â†’ æ¨ç†æ¸¬è©¦
    - é…ç½®è®Šæ›´ â†’ é‡æ–°è¨“ç·´ â†’ çµæœæ¯”è¼ƒ
```

**äº¤ä»˜ç‰©**:
- [ ] å®Œæ•´çš„æ•´åˆæ¸¬è©¦å¥—ä»¶
- [ ] æ€§èƒ½åŸºæº–æ¸¬è©¦
- [ ] å›æ­¸æ¸¬è©¦è…³æœ¬

### **4.3 æ€§èƒ½å’Œç©©å®šæ€§æ¸¬è©¦**
```python
# å‰µå»º: tests/performance/
â”œâ”€â”€ test_memory_usage.py         # è¨˜æ†¶é«”ä½¿ç”¨æ¸¬è©¦
â”œâ”€â”€ test_training_speed.py       # è¨“ç·´é€Ÿåº¦åŸºæº–
â”œâ”€â”€ test_data_loading_perf.py    # è³‡æ–™è¼‰å…¥æ€§èƒ½
â””â”€â”€ test_long_running.py         # é•·æ™‚é–“é‹è¡Œç©©å®šæ€§

def test_memory_usage():
    """è¨˜æ†¶é«”ä½¿ç”¨æ¸¬è©¦"""
    - ç›£æ§è¨“ç·´éç¨‹è¨˜æ†¶é«”ä½¿ç”¨
    - æª¢æŸ¥è¨˜æ†¶é«”æ´©æ¼
    - é©—è­‰ GPU è¨˜æ†¶é«”ç®¡ç†
```

**äº¤ä»˜ç‰©**:
- [ ] æ€§èƒ½åŸºæº–å ±å‘Š
- [ ] ç©©å®šæ€§æ¸¬è©¦çµæœ
- [ ] å„ªåŒ–å»ºè­°æ–‡æª”

---

## **éšæ®µ 5: ç°¡åŒ–éƒ¨ç½²** (å½ˆæ€§ 1-2 å¤©)

### **5.1 Hydra CLI (è‡ªå‹•åŒ–)**
```python
# train.py (Hydra è‡ªå‹• CLI)
@hydra.main(config_path="configs", config_name="training_config")
def train(cfg: DictConfig):
    """Hydra è‡ªå‹•ç”Ÿæˆ CLI"""
    # è‡ªå‹•æ”¯æ´:
    # python train.py model.d_model=512 training.batch_size=16
    # python train.py --multirun model.d_model=256,512 training.lr=1e-4,3e-4
    
    if cfg.optuna.enabled:
        run_optuna_optimization(cfg)
    else:
        run_single_training(cfg)

# ç°¡åŒ–çš„å·¥å…·è…³æœ¬
# scripts/
â”œâ”€â”€ gpu_test.py              # GPU è¨˜æ†¶é«”æ¸¬è©¦
â”œâ”€â”€ data_prep.py             # è³‡æ–™é è™•ç†
â”œâ”€â”€ model_export.py          # æ¨¡å‹åŒ¯å‡º
â””â”€â”€ docker_dev.py            # Docker é–‹ç™¼ç’°å¢ƒ
```

**äº¤ä»˜ç‰©**:
- [ ] Hydra CLI è‡ªå‹•åŒ–
- [ ] æ ¸å¿ƒå·¥å…·è…³æœ¬ (4å€‹)
- [ ] `Dockerfile.dev` é–‹ç™¼ç’°å¢ƒ

### **5.2 å·¥å…·è…³æœ¬é›†**
```python
# å‰µå»º: scripts/
â”œâ”€â”€ make_dataset.py              # Arrow åˆ†ç‰‡ç”Ÿæˆ
â”œâ”€â”€ validate_setup.py            # ç’°å¢ƒé©—è­‰
â”œâ”€â”€ benchmark_performance.py     # æ€§èƒ½åŸºæº–æ¸¬è©¦
â”œâ”€â”€ export_model.py              # æ¨¡å‹åŒ¯å‡ºå·¥å…·
â”œâ”€â”€ analyze_results.py           # çµæœåˆ†æå·¥å…·
â””â”€â”€ deploy_model.py              # æ¨¡å‹éƒ¨ç½²å·¥å…·
```

**äº¤ä»˜ç‰©**:
- [ ] å®Œæ•´çš„å·¥å…·è…³æœ¬é›†
- [ ] è…³æœ¬ä½¿ç”¨æ–‡æª”
- [ ] è‡ªå‹•åŒ–éƒ¨ç½²è…³æœ¬

---

## ğŸ“… **å„ªåŒ–å¾Œé–‹ç™¼æ™‚ç¨‹** (åŸºæ–¼ References.txt æŒ‡å°)

> **å”ä½œæ¨¡å¼**: ä¸€äººåœ˜éšŠ + AI å”ä½œï¼Œé«˜å½ˆæ€§è¿­ä»£é–‹ç™¼  
> **æŠ€è¡“é¸å‹**: Hydra + Lightning + PyArrow å„ªåŒ–æ¶æ§‹  

### **Sprint 1 (3-5å¤©): æ ¸å¿ƒåŸºç¤ + GPU é©—è­‰**
| ä»»å‹™ | å„ªå…ˆç´š | é ä¼°æ™‚é–“ | æŠ€è¡“é¸å‹å„ªåŒ– |
|------|--------|----------|-------------|
| SSOT ç›¸å®¹æ€§é©—è­‰å™¨ | P0 | 1å¤© | ä½¿ç”¨ Hydra config validation |
| GPU è³‡æºé©—è­‰ (GTX 1660 Ti) | P0 | 0.5å¤© | batch=16, seq_len=64 smoke test |
| ç‰¹å¾µè¨»å†Šç³»çµ± (Singleton) | P1 | 1å¤© | é¿å…é‡è¤‡å®šç¾©ï¼Œcore å¥—ä»¶çµ±ä¸€ç®¡ç† |
| DataLoader Smoke Test | P0 | 0.5å¤© | PyArrow + Memory Mapped File |
| Lightning æ¶æ§‹è©•ä¼° | P1 | 1å¤© | æ›¿ä»£è‡ªå¯« Trainer/Checkpoint |

### **Sprint 2 (4-6å¤©): è¨“ç·´ç®¡ç·š + RL Agent**
| ä»»å‹™ | å„ªå…ˆç´š | é ä¼°æ™‚é–“ | References.txt å»ºè­° |
|------|--------|----------|-------------------|
| Lightning Trainer æ•´åˆ | P0 | 2å¤© | çœå»è¨“ç·´è¿´åœˆé‡å¯« |
| RL Agent åŸºç¤å¯¦ä½œ | P0 | 1.5å¤© | Week 2 å¿…é ˆå®Œæˆï¼Œé¿å…èˆ‡ Optuna è¡çª |
| TSEAlphaEnv â†” SB3 ä»‹æ¥ | P1 | 1å¤© | PPO baseline policy |
| Hydra é…ç½®ç³»çµ± | P1 | 1å¤© | æ›¿ä»£è‡ªå¯« ConfigManager |
| ç«¯åˆ°ç«¯ç®¡ç·šå†’ç…™æ¸¬è©¦ | P0 | 0.5å¤© | åŒ…å« RL Agent æ¸¬è©¦ |

### **Sprint 3 (3-4å¤©): Optuna å„ªåŒ– + å¯¦é©—ç®¡ç†**
| ä»»å‹™ | å„ªå…ˆç´š | é ä¼°æ™‚é–“ | é¢¨éšªç·©è§£ |
|------|--------|----------|---------|
| Optuna + Hydra æ•´åˆ | P0 | 1.5å¤© | å…§å»º Sweep åŠŸèƒ½ |
| Mini-epoch è©•ä¼°æ©Ÿåˆ¶ | P0 | 1å¤© | é™åˆ¶ Trial è‡³ 20% è³‡æ–™ï¼Œé¿å… VRAM çˆ†ç‚¸ |
| éšæ¢¯å‹æœç´¢ç©ºé–“ | P1 | 1å¤© | seq_len, d_model åŒæ™‚èª¿æ•´æ‰¹æ¬¡ |
| Lightning Logger æ•´åˆ | P1 | 0.5å¤© | TensorBoard + CSV + JSON |

### **Sprint 4 (2-3å¤©): æ ¸å¿ƒè·¯å¾‘æ¸¬è©¦**
| ä»»å‹™ | å„ªå…ˆç´š | é ä¼°æ™‚é–“ | æ¸¬è©¦ç­–ç•¥å„ªåŒ– |
|------|--------|----------|-------------|
| æ ¸å¿ƒè·¯å¾‘æ¸¬è©¦ | P0 | 1.5å¤© | SSOTé©—è­‰ + DataLoader + Trainer + Env |
| Smoke Tests å¥—ä»¶ | P1 | 1å¤© | å¿«é€Ÿé©—è­‰ï¼Œé¿å…é•·æ™‚é–“ CI |
| Type Hint éœæ…‹æª¢æŸ¥ | P1 | 0.5å¤© | è£œå¼·è¼”åŠ©æ¨¡çµ„æ¸¬è©¦è¦†è“‹ |

### **Sprint 5 (1-2å¤©): éƒ¨ç½²æº–å‚™**
| ä»»å‹™ | å„ªå…ˆç´š | é ä¼°æ™‚é–“ | éƒ¨ç½²ç­–ç•¥ |
|------|--------|----------|---------|
| Dockerfile.dev | P1 | 0.5å¤© | GPU é©…å‹• + ä¾è³´ç’°å¢ƒ |
| CLI ä»‹é¢ (Hydra) | P0 | 1å¤© | å…§å»º override åŠŸèƒ½ |
| æ–‡æª”æ•´ç† | P1 | 0.5å¤© | ä½¿ç”¨æŒ‡å— + API æ–‡æª” |

---

## ğŸ¯ **æˆåŠŸæ¨™æº–**

### **åŠŸèƒ½å®Œæ•´æ€§**
- [ ] æ”¯æ´ SSOT è¦ç¯„çš„å®Œæ•´è¨“ç·´æµç¨‹
- [ ] ç›£ç£å­¸ç¿’ + å¼·åŒ–å­¸ç¿’ + æ··åˆè¨“ç·´æ¨¡å¼
- [ ] Optuna è¶…åƒæ•¸å„ªåŒ–æ•´åˆ
- [ ] å®Œæ•´çš„æª¢æŸ¥é»å’Œæ¢å¾©æ©Ÿåˆ¶

### **æ€§èƒ½æŒ‡æ¨™**
- [ ] è¨“ç·´é€Ÿåº¦: >100 æ¨£æœ¬/ç§’
- [ ] è¨˜æ†¶é«”ä½¿ç”¨: <16GB (180æª”è‚¡ç¥¨)
- [ ] GPU åˆ©ç”¨ç‡: >90%
- [ ] ç³»çµ±ç©©å®šæ€§: >24å°æ™‚é€£çºŒé‹è¡Œ

### **æ¸¬è©¦è¦†è“‹ç‡**
- [ ] å–®å…ƒæ¸¬è©¦: >85%
- [ ] æ•´åˆæ¸¬è©¦: >90%
- [ ] Smoke Tests: 100% é€šé
- [ ] æ€§èƒ½æ¸¬è©¦: ç¬¦åˆåŸºæº–

### **æ–‡æª”å®Œæ•´æ€§**
- [ ] API æ–‡æª”: 100% è¦†è“‹
- [ ] ä½¿ç”¨æŒ‡å—: å®Œæ•´
- [ ] é…ç½®èªªæ˜: è©³ç´°
- [ ] æ•…éšœæ’é™¤: å…¨é¢

---

## ğŸš¨ **é¢¨éšªç®¡æ§** (åŸºæ–¼ References.txt æŒ‡å°)

### **é›™ç’°å¢ƒé¢¨éšªç®¡æ§**

#### **é–‹ç™¼ç’°å¢ƒé¢¨éšª** (GTX 1660 Ti 6GB)
| é¢¨éšª | ç·©è§£ç­–ç•¥ | å¯¦ä½œæ–¹æ¡ˆ |
|------|----------|----------|
| **VRAM ä¸è¶³** | æ¥µä½é…ç½® + è³‡æ–™å­é›† | batch=8, seq_len=32, 10æª”è‚¡ç¥¨ |
| **ç…™éœ§æ¸¬è©¦è¶…æ™‚** | å¿«é€Ÿé©—è­‰æ¨¡å¼ | epoch=2, 20% è³‡æ–™ |
| **é–‹ç™¼æ•ˆç‡ä½** | è‡ªå‹•é…ç½®åˆ‡æ› | ç’°å¢ƒæª¢æ¸¬ â†’ è‡ªå‹•é¸æ“‡é…ç½® |

#### **ç”Ÿç”¢ç’°å¢ƒé¢¨éšª** (RTX 4090 24GB)
| é¢¨éšª | ç·©è§£ç­–ç•¥ | å¯¦ä½œæ–¹æ¡ˆ |
|------|----------|----------|
| **è³‡æºæµªè²»** | æœ€å¤§åŒ–åˆ©ç”¨ç­–ç•¥ | batch=128, å®Œæ•´è³‡æ–™é›† |
| **è¨“ç·´æ™‚é–“éé•·** | æ™ºèƒ½æ—©åœ + æª¢æŸ¥é» | è‡ªå‹•ä¿å­˜ + æ–·é»çºŒè¨“ |
| **è¶…åƒæ•¸æœç´¢æˆæœ¬** | åˆ†å±¤æœç´¢ç­–ç•¥ | ç²—æœç´¢ â†’ ç²¾ç´°æœç´¢ |

### **æŠ€è¡“æ•´åˆé¢¨éšª**
| é¢¨éšª | ç·©è§£ç­–ç•¥ | å¯¦ä½œæ–¹æ¡ˆ |
|------|----------|----------|
| **Lightning é·ç§»è¤‡é›œåº¦** | æ¼¸é€²å¼é·ç§» | å…ˆè©•ä¼°ï¼Œå†é€æ­¥æ›¿æ›ç¾æœ‰ Trainer |
| **Hydra é…ç½®è¡çª** | çµ±ä¸€é…ç½®å…¥å£ | å–®ä¸€ config.yamlï¼Œé¿å…å¤šè™•å®šç¾© |
| **RL Agent æ•´åˆå»¶é²** | æå‰åˆ° Sprint 2 | é¿å…èˆ‡ Optuna ä½µç™¼è¡çª |

### **å”ä½œæ•ˆç‡é¢¨éšª**
| é¢¨éšª | ç·©è§£ç­–ç•¥ | å¯¦ä½œæ–¹æ¡ˆ |
|------|----------|----------|
| **Singleton æ¨¡çµ„è¡çª** | æ˜ç¢ºç´„å®š | FeaturesRegistry åªèƒ½ç”± core å¥—ä»¶ import |
| **æ¸¬è©¦è¦†è“‹ç‡éé«˜** | èšç„¦æ ¸å¿ƒè·¯å¾‘ | 85%/90% â†’ æ ¸å¿ƒè·¯å¾‘ 100% + è¼”åŠ©æ¨¡çµ„ Type Hint |
| **CI æ™‚é–“éé•·** | åˆ†å±¤æ¸¬è©¦ç­–ç•¥ | Smoke Tests å¿«é€Ÿé©—è­‰ï¼ŒIntegration Tests nightly |

---

## ğŸ¯ **ä¸€äººåœ˜éšŠå”ä½œç­–ç•¥**

### **å½ˆæ€§é–‹ç™¼æ¨¡å¼**
```
è¿­ä»£é€±æœŸ: 2-3å¤© mini-sprints
â”œâ”€â”€ æ¯æ—¥åŒæ­¥: 30åˆ†é˜é€²åº¦æª¢è¦– + å•é¡Œè¨è«–
â”œâ”€â”€ æŠ€è¡“æ±ºç­–: å³æ™‚èª¿æ•´ï¼Œç„¡éœ€å†—é•·æœƒè­°
â”œâ”€â”€ æ¸¬è©¦ç­–ç•¥: é‚Šé–‹ç™¼é‚Šæ¸¬è©¦ï¼Œå¿«é€Ÿé©—è­‰
â””â”€â”€ æ–‡æª”æ›´æ–°: å¯¦æ™‚æ›´æ–°ï¼Œä¿æŒåŒæ­¥
```

### **AI å”ä½œåˆ†å·¥**
| é–‹ç™¼è€…è² è²¬ | AI å”åŠ© | å”ä½œæ–¹å¼ |
|------------|---------|----------|
| æ¶æ§‹è¨­è¨ˆæ±ºç­– | ç¨‹å¼ç¢¼å¯¦ä½œå»ºè­° | è¨è«– â†’ å¯¦ä½œ â†’ é©—è­‰ |
| æ¥­å‹™é‚è¼¯é©—è­‰ | æŠ€è¡“å¯¦ä½œç´°ç¯€ | éœ€æ±‚ â†’ ç¨‹å¼ç¢¼ â†’ æ¸¬è©¦ |
| ç³»çµ±æ•´åˆæ¸¬è©¦ | å–®å…ƒæ¸¬è©¦ç”Ÿæˆ | æ‰‹å‹• â†’ è‡ªå‹• â†’ é©—è­‰ |
| æ€§èƒ½èª¿å„ª | ç¨‹å¼ç¢¼å„ªåŒ–å»ºè­° | åˆ†æ â†’ å„ªåŒ– â†’ åŸºæº– |

### **ç«‹å³è¡Œå‹•å»ºè­°** (å„ªå…ˆç´šæ’åº)

#### **ğŸš€ Phase 1: å¿«é€Ÿé©—è­‰ (1-2å¤©)**
1. **é›™ç¡¬é«”ç’°å¢ƒé©—è­‰** - ç¢ºèªå…©ç¨®é…ç½®å¯ç”¨æ€§
   ```bash
   # GTX 1660 Ti ç…™éœ§æ¸¬è©¦
   python scripts/smoke_test_gtx1660ti.py
   
   # RTX 4090 ç’°å¢ƒæª¢æŸ¥ (å¦‚æœå¯ç”¨)
   python scripts/full_training_rtx4090.py --mode supervised --epochs 1 --force
   ```

2. **ç¡¬é«”é…ç½®ç³»çµ±æ¸¬è©¦** - è‡ªå‹•æª¢æ¸¬èˆ‡é…ç½®åˆ‡æ›
   ```python
   # æ¸¬è©¦ç¡¬é«”æª¢æ¸¬
   python configs/hardware_configs.py
   
   # æ¸¬è©¦é…ç½®åˆ‡æ›
   python -c "from configs.hardware_configs import ConfigManager; print(ConfigManager.get_auto_config())"
   ```

3. **SSOT ç›¸å®¹æ€§æª¢æŸ¥** - é©—è­‰ç¾æœ‰å¯¦ä½œ
   ```python
   # å‰µå»ºå¿«é€Ÿé©—è­‰è…³æœ¬
   scripts/quick_ssot_check.py
   ```

#### **ğŸ”§ Phase 2: æŠ€è¡“é¸å‹é©—è­‰ (2-3å¤©)**
1. **Lightning é·ç§»è©•ä¼°** - è©•ä¼°ç¾æœ‰ Trainer é·ç§»æˆæœ¬
2. **Hydra é…ç½®æ•´åˆ** - æ›¿æ›ç¾æœ‰é…ç½®ç³»çµ±
3. **PyArrow æ€§èƒ½æ¸¬è©¦** - é©—è­‰ I/O å„ªåŒ–æ•ˆæœ

#### **âš¡ Phase 3: æ ¸å¿ƒåŠŸèƒ½å¯¦ä½œ (3-4å¤©)**
1. **RL Agent åŸºç¤å¯¦ä½œ** - SB3 + TSEAlphaEnv æ•´åˆ
2. **Mini-epoch æ©Ÿåˆ¶** - Optuna Trial åŠ é€Ÿ
3. **ç«¯åˆ°ç«¯å†’ç…™æ¸¬è©¦** - å®Œæ•´æµç¨‹é©—è­‰

---

## ğŸ“š **åƒè€ƒæ–‡æª”**

- **åŸºæº–è¦ç¯„**: `training_module_ssot.md`
- **å¯¦ä½œæŒ‡å°**: `References.txt` â­ (æ–°å¢)
- **è‚¡ç¥¨é…ç½®**: `stock_split_config.json`
- **è³‡æ–™çµæ§‹**: `db_structure.json`
- **ç³»çµ±æ¶æ§‹**: `docs/TECHNICAL_GUIDE.md`
- **é–‹ç™¼æ­·ç¨‹**: `docs/DEVELOPMENT_LOG.md`

---

## ğŸš€ **ä¸‹ä¸€æ­¥è¡Œå‹•æ±ºç­–**

### **ç«‹å³å¯åŸ·è¡Œé¸é …**

#### **é¸é … A: é›™ç¡¬é«”é©—è­‰è·¯ç·š** âš¡ (æ¨è–¦)
- **æ™‚é–“**: 1-2å¤©
- **ç›®æ¨™**: ç¢ºèªé›™ç¡¬é«”ç’°å¢ƒå¯è¡Œæ€§
- **GTX 1660 Ti**: ç…™éœ§æ¸¬è©¦ + ä½é…ç½®é©—è­‰
- **RTX 4090**: é«˜é…ç½®æ¸¬è©¦ + æ€§èƒ½åŸºæº–
- **äº¤ä»˜**: ç¡¬é«”é…ç½®ç³»çµ± + è‡ªå‹•åˆ‡æ›æ©Ÿåˆ¶

#### **é¸é … B: é–‹ç™¼ç’°å¢ƒå„ªåŒ–è·¯ç·š** ğŸ”§  
- **æ™‚é–“**: 2-3å¤©
- **ç›®æ¨™**: å®Œæˆé–‹ç™¼å·¥å…·éˆ
- **é‡é»**: Lightningé·ç§» + Hydraé…ç½® + ç…™éœ§æ¸¬è©¦å¥—ä»¶
- **äº¤ä»˜**: é–‹ç™¼æ•ˆç‡å·¥å…· + å¿«é€Ÿè¿­ä»£èƒ½åŠ›

#### **é¸é … C: ç”Ÿç”¢è¨“ç·´è·¯ç·š** ğŸ’ª
- **æ™‚é–“**: 3-5å¤©  
- **ç›®æ¨™**: å®Œæ•´è¨“ç·´ç³»çµ±
- **é‡é»**: å¤§è¦æ¨¡è¨“ç·´ + Optunaå„ªåŒ– + æª¢æŸ¥é»ç®¡ç†
- **äº¤ä»˜**: ç”Ÿç”¢ç´šè¨“ç·´ç®¡ç·š + è¶…åƒæ•¸å„ªåŒ–

### **å»ºè­°é–‹å§‹é»**
**æ¨è–¦é¸é … A** - é›™ç¡¬é«”é©—è­‰è·¯ç·šï¼Œç¢ºä¿å…©ç¨®ç’°å¢ƒéƒ½èƒ½æ­£å¸¸å·¥ä½œ

---

## ğŸ“‹ **å”ä½œæª¢æŸ¥æ¸…å–®**

### **æ¯æ—¥åŒæ­¥æª¢æŸ¥**
- [ ] ç•¶å‰ä»»å‹™é€²åº¦ç¢ºèª
- [ ] æŠ€è¡“å•é¡Œè¨è«–è§£æ±º  
- [ ] ä¸‹ä¸€æ­¥è¨ˆç•«èª¿æ•´
- [ ] é¢¨éšªé»è­˜åˆ¥ç·©è§£

### **Sprint å®Œæˆæª¢æŸ¥**
- [ ] åŠŸèƒ½é©—è­‰é€šé
- [ ] ç¨‹å¼ç¢¼å“è³ªæª¢æŸ¥
- [ ] æ–‡æª”åŒæ­¥æ›´æ–°
- [ ] ä¸‹å€‹ Sprint è¦åŠƒ

---

**æ–‡æª”ç‰ˆæœ¬**: v2.0 (References.txt å„ªåŒ–ç‰ˆ)  
**æœ€å¾Œæ›´æ–°**: 2025-01-15  
**å”ä½œæ¨¡å¼**: ä¸€äººåœ˜éšŠ + AI é«˜å½ˆæ€§è¿­ä»£  
**ä¸‹æ¬¡å¯©æŸ¥**: æ¯å€‹ mini-sprint å®Œæˆå¾Œ
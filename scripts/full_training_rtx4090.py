#!/usr/bin/env python3
"""
TSE Alpha å®Œæ•´è¨“ç·´è…³æœ¬ - RTX 4090 å°ˆç”¨
é«˜é…ç½®å¤§è¦æ¨¡è¨“ç·´
"""
import sys
import os
import time
import argparse
from pathlib import Path
import torch
import logging
from datetime import datetime

# æ·»åŠ è·¯å¾‘
sys.path.append(str(Path(__file__).parent.parent))

# å¼·åˆ¶ä½¿ç”¨ç”Ÿç”¢é…ç½®
os.environ['TSE_ALPHA_MODE'] = 'production'
os.environ['CUDA_VISIBLE_DEVICES'] = '0'

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'logs/training_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def check_rtx4090_environment():
    """æª¢æŸ¥ RTX 4090 ç’°å¢ƒ"""
    logger.info("ğŸ” æª¢æŸ¥ RTX 4090 ç’°å¢ƒ...")
    
    if not torch.cuda.is_available():
        logger.error("âŒ CUDA ä¸å¯ç”¨")
        return False
    
    gpu_props = torch.cuda.get_device_properties(0)
    gpu_memory = gpu_props.total_memory / 1e9
    
    logger.info(f"GPU: {gpu_props.name}")
    logger.info(f"VRAM: {gpu_memory:.1f}GB")
    logger.info(f"è¨ˆç®—èƒ½åŠ›: {gpu_props.major}.{gpu_props.minor}")
    
    # æª¢æŸ¥æ˜¯å¦ç‚º RTX 4090 æˆ–é«˜éš GPU
    if '4090' in gpu_props.name or gpu_memory > 20:
        logger.info("âœ… æª¢æ¸¬åˆ° RTX 4090 æˆ–åŒç­‰ç´š GPUï¼Œä½¿ç”¨é«˜é…ç½®æ¨¡å¼")
        return True
    else:
        logger.warning(f"âš ï¸  é RTX 4090 ({gpu_props.name}, {gpu_memory:.1f}GB)")
        logger.warning("âš ï¸  å»ºè­°ä½¿ç”¨ RTX 4090 é€²è¡Œå®Œæ•´è¨“ç·´")
        
        # è©¢å•æ˜¯å¦ç¹¼çºŒ
        response = input("æ˜¯å¦ç¹¼çºŒä½¿ç”¨ç•¶å‰ GPUï¼Ÿ(y/N): ")
        return response.lower() == 'y'

def create_production_config(args):
    """å‰µå»ºç”Ÿç”¢ç’°å¢ƒé…ç½®"""
    from configs.hardware_configs import ConfigManager
    
    # ç²å– RTX 4090 é…ç½®
    hw_config = ConfigManager.get_config('rtx4090')
    
    # æ ¹æ“šå‘½ä»¤è¡Œåƒæ•¸èª¿æ•´
    config = {
        # ç¡¬é«”é…ç½®
        'hardware_profile': hw_config.name,
        'batch_size': args.batch_size or hw_config.batch_size,
        'accumulate_grad_batches': hw_config.accumulate_grad_batches,
        'precision': hw_config.precision,
        'num_workers': hw_config.num_workers,
        'pin_memory': hw_config.pin_memory,
        
        # æ¨¡å‹é…ç½®
        'sequence_length': hw_config.seq_len,
        'n_stocks': args.n_stocks or hw_config.n_stocks,
        
        # è¨“ç·´é…ç½®
        'num_epochs': args.epochs or hw_config.num_epochs,
        'data_subset_ratio': args.data_ratio or hw_config.data_subset_ratio,
        'learning_rate': args.lr or (1e-4 * hw_config.batch_size / 64),
        
        # é«˜éšé…ç½®
        'early_stopping_patience': 30,
        'save_top_k': 5,
        'log_every_n_steps': 50,
        'val_check_interval': 0.25,  # æ¯ 25% epoch é©—è­‰ä¸€æ¬¡
        
        # Optuna é…ç½® (å¦‚æœå•Ÿç”¨)
        'optuna_trials': args.optuna_trials or 100,
        'optuna_timeout': args.optuna_timeout or 3600 * 24,  # 24å°æ™‚
        
        # æª¢æŸ¥é»é…ç½®
        'checkpoint_every_n_epochs': 10,
        'auto_resume': True,
        
        # è³‡æ–™é…ç½®
        'train_start_date': '2020-03-02',
        'train_end_date': '2023-12-31',
        'val_start_date': '2024-01-01',
        'val_end_date': '2024-06-30',
        'test_start_date': '2024-07-01',
        'test_end_date': '2024-12-31'
    }
    
    logger.info("ç”Ÿç”¢é…ç½®:")
    for key, value in config.items():
        logger.info(f"  {key}: {value}")
    
    return config

def setup_training_environment(config):
    """è¨­ç½®è¨“ç·´ç’°å¢ƒ"""
    logger.info("ğŸ”§ è¨­ç½®è¨“ç·´ç’°å¢ƒ...")
    
    try:
        # å°å…¥å¿…è¦æ¨¡çµ„
        from models.config.training_config import TrainingConfig
        from models.model_architecture import TSEAlphaModel, ModelConfig
        from models.trainer import ModelTrainer
        from models.data_loader import TSEDataLoader, DataConfig
        from stock_config import TRAIN_STOCKS, VALIDATION_STOCKS, TEST_STOCKS
        
        # å‰µå»ºè¨“ç·´é…ç½®
        training_config = TrainingConfig()
        training_config.batch_size = config['batch_size']
        training_config.num_epochs = config['num_epochs']
        training_config.learning_rate = config['learning_rate']
        training_config.early_stopping_patience = config['early_stopping_patience']
        
        # å‰µå»ºæ¨¡å‹é…ç½®
        model_config = ModelConfig(
            price_frame_shape=(config['n_stocks'], config['sequence_length'], 27),
            fundamental_dim=43,  # å®Œæ•´åŸºæœ¬é¢ç‰¹å¾µ
            n_stocks=config['n_stocks'],
            hidden_dim=512,      # é«˜é…ç½®
            num_heads=16,        # æ›´å¤šæ³¨æ„åŠ›é ­
            num_layers=6,        # æ›´æ·±å±¤æ•¸
            dropout=0.1
        )
        
        # å‰µå»ºè³‡æ–™é…ç½®
        symbols = TRAIN_STOCKS + VALIDATION_STOCKS + TEST_STOCKS
        if config['n_stocks'] < 180:
            symbols = symbols[:config['n_stocks']]
        
        data_config = DataConfig(
            symbols=symbols,
            train_start_date=config['train_start_date'],
            train_end_date=config['train_end_date'],
            val_start_date=config['val_start_date'],
            val_end_date=config['val_end_date'],
            test_start_date=config['test_start_date'],
            test_end_date=config['test_end_date'],
            sequence_length=config['sequence_length'],
            batch_size=config['batch_size'],
            num_workers=config['num_workers']
        )
        
        # å‰µå»ºæ¨¡å‹
        model = TSEAlphaModel(model_config)
        
        # å‰µå»ºè¨“ç·´å™¨
        trainer = ModelTrainer(model, training_config, data_config)
        
        # å‰µå»ºè³‡æ–™è¼‰å…¥å™¨
        data_loader = TSEDataLoader(data_config)
        
        logger.info("âœ… è¨“ç·´ç’°å¢ƒè¨­ç½®å®Œæˆ")
        
        return {
            'model': model,
            'trainer': trainer,
            'data_loader': data_loader,
            'training_config': training_config,
            'model_config': model_config,
            'data_config': data_config
        }
        
    except Exception as e:
        logger.error(f"âŒ è¨“ç·´ç’°å¢ƒè¨­ç½®å¤±æ•—: {e}")
        raise

def run_supervised_training(components, config):
    """åŸ·è¡Œç›£ç£å­¸ç¿’è¨“ç·´"""
    logger.info("ğŸš€ é–‹å§‹ç›£ç£å­¸ç¿’è¨“ç·´...")
    
    try:
        trainer = components['trainer']
        data_loader = components['data_loader']
        
        # æº–å‚™è³‡æ–™
        data_loader.prepare_data()
        train_loader, val_loader, test_loader = data_loader.get_dataloaders()
        
        logger.info(f"è¨“ç·´é›†: {len(train_loader)} æ‰¹æ¬¡")
        logger.info(f"é©—è­‰é›†: {len(val_loader)} æ‰¹æ¬¡")
        logger.info(f"æ¸¬è©¦é›†: {len(test_loader)} æ‰¹æ¬¡")
        
        # é–‹å§‹è¨“ç·´
        start_time = time.time()
        results = trainer.train_supervised(train_loader, val_loader, test_loader)
        training_time = time.time() - start_time
        
        logger.info(f"âœ… ç›£ç£å­¸ç¿’è¨“ç·´å®Œæˆ")
        logger.info(f"è¨“ç·´æ™‚é–“: {training_time/3600:.2f} å°æ™‚")
        logger.info(f"æœ€ä½³é©—è­‰ Loss: {results['best_val_loss']:.6f}")
        
        return results
        
    except Exception as e:
        logger.error(f"âŒ ç›£ç£å­¸ç¿’è¨“ç·´å¤±æ•—: {e}")
        raise

def run_optuna_optimization(components, config):
    """åŸ·è¡Œ Optuna è¶…åƒæ•¸å„ªåŒ–"""
    logger.info("ğŸ” é–‹å§‹ Optuna è¶…åƒæ•¸å„ªåŒ–...")
    
    try:
        import optuna
        from training.optuna_optimizer import OptunaOptimizer
        
        # å‰µå»º Optuna å„ªåŒ–å™¨
        optimizer = OptunaOptimizer(components['training_config'])
        
        # å‰µå»ºç ”ç©¶
        study = optimizer.create_study()
        
        # åŸ·è¡Œå„ªåŒ–
        study.optimize(
            optimizer.objective,
            n_trials=config['optuna_trials'],
            timeout=config['optuna_timeout']
        )
        
        logger.info(f"âœ… Optuna å„ªåŒ–å®Œæˆ")
        logger.info(f"æœ€ä½³åƒæ•¸: {study.best_params}")
        logger.info(f"æœ€ä½³åˆ†æ•¸: {study.best_value:.6f}")
        
        return study.best_params
        
    except Exception as e:
        logger.error(f"âŒ Optuna å„ªåŒ–å¤±æ•—: {e}")
        raise

def main():
    """ä¸»è¦è¨“ç·´æµç¨‹"""
    parser = argparse.ArgumentParser(description='TSE Alpha å®Œæ•´è¨“ç·´ - RTX 4090')
    parser.add_argument('--mode', choices=['supervised', 'optuna', 'both'], default='supervised',
                       help='è¨“ç·´æ¨¡å¼')
    parser.add_argument('--epochs', type=int, help='è¨“ç·´è¼ªæ•¸')
    parser.add_argument('--batch-size', type=int, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--lr', type=float, help='å­¸ç¿’ç‡')
    parser.add_argument('--n-stocks', type=int, help='è‚¡ç¥¨æ•¸é‡')
    parser.add_argument('--data-ratio', type=float, help='è³‡æ–™ä½¿ç”¨æ¯”ä¾‹')
    parser.add_argument('--optuna-trials', type=int, help='Optuna è©¦é©—æ¬¡æ•¸')
    parser.add_argument('--optuna-timeout', type=int, help='Optuna è¶…æ™‚æ™‚é–“(ç§’)')
    parser.add_argument('--resume', type=str, help='å¾æª¢æŸ¥é»æ¢å¾©')
    parser.add_argument('--force', action='store_true', help='å¼·åˆ¶åŸ·è¡Œ(è·³é GPU æª¢æŸ¥)')
    
    args = parser.parse_args()
    
    logger.info("=" * 60)
    logger.info("ğŸš€ TSE Alpha å®Œæ•´è¨“ç·´ - RTX 4090")
    logger.info("=" * 60)
    
    # æª¢æŸ¥ç’°å¢ƒ
    if not args.force and not check_rtx4090_environment():
        logger.error("âŒ ç’°å¢ƒæª¢æŸ¥å¤±æ•—")
        return False
    
    # å‰µå»ºé…ç½®
    config = create_production_config(args)
    
    # è¨­ç½®è¨“ç·´ç’°å¢ƒ
    components = setup_training_environment(config)
    
    # åŸ·è¡Œè¨“ç·´
    try:
        if args.mode in ['supervised', 'both']:
            supervised_results = run_supervised_training(components, config)
        
        if args.mode in ['optuna', 'both']:
            optuna_results = run_optuna_optimization(components, config)
        
        logger.info("ğŸ‰ å®Œæ•´è¨“ç·´æµç¨‹å®Œæˆï¼")
        return True
        
    except Exception as e:
        logger.error(f"âŒ è¨“ç·´æµç¨‹å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
#!/usr/bin/env python3
"""
TSE Alpha ä»£ç†äººè¡Œç‚ºæ¸¬è©¦è…³æœ¬
æ¸¬è©¦æ±ºç­–é‚è¼¯ã€é¢¨éšªç®¡ç†å’Œäº¤æ˜“ç­–ç•¥è¡Œç‚º
"""

import sys
import os
from pathlib import Path
import numpy as np
import torch
import torch.nn.functional as F
from typing import Dict, Any, List, Tuple
import traceback

# æ·»åŠ è·¯å¾‘
sys.path.append(str(Path(__file__).parent))

class SimpleAgent:
    """ç°¡å–®æ¸¬è©¦ä»£ç†äºº"""
    
    def __init__(self, model, risk_tolerance=0.5):
        self.model = model
        self.risk_tolerance = risk_tolerance
        self.action_history = []
        self.performance_history = []
    
    def get_action(self, observation, info=None):
        """ç²å–å‹•ä½œ"""
        # ä½¿ç”¨æ¨¡å‹ç”ŸæˆåŸºç¤å‹•ä½œ
        base_action = self.model.get_action(observation, deterministic=False)
        
        # æ‡‰ç”¨é¢¨éšªç®¡ç†
        risk_adjusted_action = self.apply_risk_management(base_action, observation, info)
        
        # è¨˜éŒ„å‹•ä½œ
        self.action_history.append(risk_adjusted_action)
        
        return risk_adjusted_action
    
    def apply_risk_management(self, action, observation, info):
        """æ‡‰ç”¨é¢¨éšªç®¡ç†"""
        stock_idx, position_array = action
        position_qty = position_array[0]
        
        # ç²å–é¢¨éšªè©•åˆ†
        with torch.no_grad():
            outputs = self.model(observation)
            risk_score = outputs['risk_score'].item()
        
        # æ ¹æ“šé¢¨éšªèª¿æ•´å€‰ä½
        if risk_score > self.risk_tolerance:
            # é«˜é¢¨éšªæ™‚æ¸›å°‘å€‰ä½
            position_qty = int(position_qty * (1 - risk_score))
        
        # å¸³æˆ¶ç‹€æ…‹æª¢æŸ¥
        account_state = observation['account'][0]  # å–ç¬¬ä¸€å€‹æ¨£æœ¬
        nav_norm, pos_ratio, unreal_pnl_pct, risk_buffer = account_state
        
        # å¦‚æœæŒå€‰æ¯”ä¾‹éé«˜ï¼Œæ¸›å°‘æ–°å€‰ä½
        if pos_ratio > 0.8:
            position_qty = int(position_qty * 0.5)
        
        # å¦‚æœæœªå¯¦ç¾æç›Šç‚ºè² ï¼Œæ›´ä¿å®ˆ
        if unreal_pnl_pct < -0.05:  # æå¤±è¶…é5%
            position_qty = int(position_qty * 0.3)
        
        return (stock_idx, np.array([position_qty], dtype=np.int16))
    
    def update_performance(self, reward, info):
        """æ›´æ–°ç¸¾æ•ˆè¨˜éŒ„"""
        self.performance_history.append({
            'reward': reward,
            'nav': info.get('nav', 0),
            'positions': len(info.get('positions', {})),
            'cash': info.get('cash', 0)
        })

def test_agent_creation():
    """æ¸¬è©¦ 1: ä»£ç†äººå‰µå»ºå’Œåˆå§‹åŒ–"""
    print("ğŸ¤– æ¸¬è©¦ 1: ä»£ç†äººå‰µå»ºå’Œåˆå§‹åŒ–")
    print("-" * 40)
    
    try:
        from models.config.training_config import TrainingConfig
        from models.model_architecture import ModelConfig, TSEAlphaModel
        
        # å‰µå»ºé…ç½®
        training_config = TrainingConfig()
        model_config = ModelConfig(
            price_frame_shape=(5, 64, training_config.price_features),
            fundamental_dim=training_config.fundamental_features,
            account_dim=training_config.account_features,
            n_stocks=5,
            hidden_dim=128
        )
        
        # å‰µå»ºæ¨¡å‹
        model = TSEAlphaModel(model_config)
        
        # å‰µå»ºä»£ç†äºº
        agent = SimpleAgent(model, risk_tolerance=0.6)
        
        print("âœ… ä»£ç†äººå‰µå»ºæˆåŠŸ")
        print(f"   é¢¨éšªå®¹å¿åº¦: {agent.risk_tolerance}")
        print(f"   æ¨¡å‹åƒæ•¸æ•¸: {sum(p.numel() for p in model.parameters()):,}")
        
        return True, agent, model_config, training_config
        
    except Exception as e:
        print(f"âŒ ä»£ç†äººå‰µå»ºå¤±æ•—: {str(e)}")
        traceback.print_exc()
        return False, None, None, None

def test_decision_consistency(agent, model_config):
    """æ¸¬è©¦ 2: æ±ºç­–ä¸€è‡´æ€§"""
    print("\nğŸ§  æ¸¬è©¦ 2: æ±ºç­–ä¸€è‡´æ€§")
    print("-" * 40)
    
    try:
        # å‰µå»ºå›ºå®šè§€æ¸¬
        observation = {
            'price_frame': torch.randn(1, model_config.n_stocks, 64, model_config.price_frame_shape[2]),
            'fundamental': torch.randn(1, model_config.fundamental_dim),
            'account': torch.tensor([[0.0, 0.5, 0.02, 0.8]], dtype=torch.float32)  # æ­£å¸¸ç‹€æ…‹
        }
        
        # æ¸¬è©¦å¤šæ¬¡æ±ºç­–
        actions = []
        for i in range(10):
            action = agent.get_action(observation)
            actions.append(action)
        
        print("âœ… æ±ºç­–ç”ŸæˆæˆåŠŸ")
        print(f"   ç”Ÿæˆ {len(actions)} å€‹æ±ºç­–")
        
        # åˆ†ææ±ºç­–åˆ†å¸ƒ
        stock_indices = [action[0] for action in actions]
        position_sizes = [action[1][0] for action in actions]
        
        print(f"   è‚¡ç¥¨é¸æ“‡ç¯„åœ: {min(stock_indices)} - {max(stock_indices)}")
        print(f"   å€‰ä½å¤§å°ç¯„åœ: {min(position_sizes)} - {max(position_sizes)}")
        print(f"   å¹³å‡å€‰ä½å¤§å°: {np.mean(position_sizes):.1f}")
        
        # æª¢æŸ¥æ±ºç­–åˆç†æ€§
        assert all(0 <= idx < model_config.n_stocks for idx in stock_indices), "è‚¡ç¥¨ç´¢å¼•è¶…å‡ºç¯„åœ"
        assert all(-model_config.max_position <= pos <= model_config.max_position 
                  for pos in position_sizes), "å€‰ä½å¤§å°è¶…å‡ºé™åˆ¶"
        
        print("âœ… æ±ºç­–åˆç†æ€§é©—è­‰é€šé")
        
        return True, observation
        
    except Exception as e:
        print(f"âŒ æ±ºç­–ä¸€è‡´æ€§æ¸¬è©¦å¤±æ•—: {str(e)}")
        traceback.print_exc()
        return False, None

def test_risk_management(agent, observation, model_config):
    """æ¸¬è©¦ 3: é¢¨éšªç®¡ç†æ©Ÿåˆ¶"""
    print("\nğŸ›¡ï¸ æ¸¬è©¦ 3: é¢¨éšªç®¡ç†æ©Ÿåˆ¶")
    print("-" * 40)
    
    try:
        # æ¸¬è©¦ä¸åŒé¢¨éšªæƒ…å¢ƒ
        risk_scenarios = [
            {
                'name': 'æ­£å¸¸ç‹€æ…‹',
                'account': torch.tensor([[0.0, 0.3, 0.01, 0.9]], dtype=torch.float32),
                'expected_behavior': 'æ­£å¸¸äº¤æ˜“'
            },
            {
                'name': 'é«˜æŒå€‰æ¯”ä¾‹',
                'account': torch.tensor([[0.0, 0.9, 0.01, 0.5]], dtype=torch.float32),
                'expected_behavior': 'æ¸›å°‘å€‰ä½'
            },
            {
                'name': 'å¤§å¹…è™§æ',
                'account': torch.tensor([[0.0, 0.5, -0.08, 0.3]], dtype=torch.float32),
                'expected_behavior': 'ä¿å®ˆäº¤æ˜“'
            },
            {
                'name': 'é¢¨éšªç·©è¡ä¸è¶³',
                'account': torch.tensor([[0.0, 0.7, -0.03, 0.1]], dtype=torch.float32),
                'expected_behavior': 'è¬¹æ…äº¤æ˜“'
            }
        ]
        
        scenario_results = []
        
        for scenario in risk_scenarios:
            print(f"\n   æ¸¬è©¦æƒ…å¢ƒ: {scenario['name']}")
            
            # ä¿®æ”¹è§€æ¸¬ä¸­çš„å¸³æˆ¶ç‹€æ…‹
            test_observation = {
                'price_frame': observation['price_frame'].clone(),
                'fundamental': observation['fundamental'].clone(),
                'account': scenario['account']
            }
            
            # ç”Ÿæˆå¤šå€‹æ±ºç­–
            actions = []
            for _ in range(5):
                action = agent.get_action(test_observation)
                actions.append(action[1][0])  # å€‰ä½å¤§å°
            
            avg_position = np.mean(actions)
            print(f"     å¹³å‡å€‰ä½å¤§å°: {avg_position:.1f}")
            print(f"     æœŸæœ›è¡Œç‚º: {scenario['expected_behavior']}")
            
            scenario_results.append({
                'scenario': scenario['name'],
                'avg_position': avg_position,
                'positions': actions
            })
        
        # é©—è­‰é¢¨éšªç®¡ç†æ•ˆæœ
        normal_pos = scenario_results[0]['avg_position']
        high_exposure_pos = scenario_results[1]['avg_position']
        loss_pos = scenario_results[2]['avg_position']
        
        print(f"\n   é¢¨éšªç®¡ç†æ•ˆæœåˆ†æ:")
        print(f"     æ­£å¸¸ç‹€æ…‹å¹³å‡å€‰ä½: {normal_pos:.1f}")
        print(f"     é«˜æŒå€‰æ™‚å¹³å‡å€‰ä½: {high_exposure_pos:.1f}")
        print(f"     è™§ææ™‚å¹³å‡å€‰ä½: {loss_pos:.1f}")
        
        # é©—è­‰é¢¨éšªç®¡ç†é‚è¼¯
        risk_reduction_1 = high_exposure_pos <= normal_pos
        risk_reduction_2 = loss_pos <= normal_pos
        
        print(f"     é«˜æŒå€‰æ™‚æ¸›å°‘å€‰ä½: {'âœ…' if risk_reduction_1 else 'âŒ'}")
        print(f"     è™§ææ™‚æ¸›å°‘å€‰ä½: {'âœ…' if risk_reduction_2 else 'âŒ'}")
        
        if risk_reduction_1 and risk_reduction_2:
            print("âœ… é¢¨éšªç®¡ç†æ©Ÿåˆ¶æ­£å¸¸é‹ä½œ")
            return True
        else:
            print("âš ï¸ é¢¨éšªç®¡ç†æ©Ÿåˆ¶å¯èƒ½éœ€è¦èª¿æ•´")
            return True  # ä»ç®—é€šéï¼Œä½†éœ€è¦æ³¨æ„
        
    except Exception as e:
        print(f"âŒ é¢¨éšªç®¡ç†æ¸¬è©¦å¤±æ•—: {str(e)}")
        traceback.print_exc()
        return False

def test_trading_strategy(agent, model_config, training_config):
    """æ¸¬è©¦ 4: äº¤æ˜“ç­–ç•¥è¡Œç‚º"""
    print("\nğŸ“ˆ æ¸¬è©¦ 4: äº¤æ˜“ç­–ç•¥è¡Œç‚º")
    print("-" * 40)
    
    try:
        from gym_env.env import TSEAlphaEnv
        
        # å‰µå»ºç’°å¢ƒ
        env = TSEAlphaEnv(
            symbols=['2330', '2317', '2454'],
            start_date='2024-01-01',
            end_date='2024-01-15',  # è¼ƒçŸ­æœŸé–“
            initial_cash=1000000.0
        )
        
        # é‡ç½®ç’°å¢ƒ
        observation, info = env.reset(seed=42)
        
        print("âœ… äº¤æ˜“ç’°å¢ƒå‰µå»ºæˆåŠŸ")
        
        # åŸ·è¡Œäº¤æ˜“ç­–ç•¥
        total_reward = 0.0
        step_count = 0
        max_steps = 15
        
        print("\n   åŸ·è¡Œäº¤æ˜“ç­–ç•¥...")
        
        for step in range(max_steps):
            # èª¿æ•´è§€æ¸¬æ ¼å¼
            n_stocks = model_config.n_stocks
            env_price_frame = observation['price_frame']
            
            if env_price_frame.shape[0] != n_stocks:
                if env_price_frame.shape[0] < n_stocks:
                    padding = np.zeros((n_stocks - env_price_frame.shape[0], 64, 5), dtype=np.float32)
                    adjusted_price_frame = np.concatenate([env_price_frame, padding], axis=0)
                else:
                    adjusted_price_frame = env_price_frame[:n_stocks]
            else:
                adjusted_price_frame = env_price_frame
            
            model_observation = {
                'price_frame': torch.tensor(adjusted_price_frame).unsqueeze(0),
                'fundamental': torch.tensor(observation['fundamental']).unsqueeze(0),
                'account': torch.tensor(observation['account']).unsqueeze(0)
            }
            
            # ä»£ç†äººæ±ºç­–
            action = agent.get_action(model_observation, info)
            
            # èª¿æ•´å‹•ä½œçµ¦ç’°å¢ƒ
            stock_idx, position_array = action
            env_n_stocks = len(env.symbols)
            if stock_idx >= env_n_stocks:
                stock_idx = stock_idx % env_n_stocks
            
            adjusted_action = (stock_idx, position_array)
            
            # åŸ·è¡Œå‹•ä½œ
            observation, reward, terminated, truncated, info = env.step(adjusted_action)
            
            # æ›´æ–°ä»£ç†äººç¸¾æ•ˆ
            agent.update_performance(reward, info)
            
            total_reward += reward
            step_count += 1
            
            if step % 5 == 0:
                nav = info.get('nav', 0)
                positions = len(info.get('positions', {}))
                print(f"     æ­¥é©Ÿ {step+1}: NAV={nav:,.0f}, æŒå€‰={positions}æª”, "
                      f"ç´¯ç©çå‹µ={total_reward:.4f}")
            
            if terminated or truncated:
                break
        
        print("âœ… äº¤æ˜“ç­–ç•¥åŸ·è¡Œå®Œæˆ")
        
        # åˆ†æäº¤æ˜“è¡Œç‚º
        final_nav = info.get('nav', 0)
        initial_nav = 1000000.0
        total_return = (final_nav - initial_nav) / initial_nav
        
        print(f"\n   äº¤æ˜“ç¸¾æ•ˆåˆ†æ:")
        print(f"     åˆå§‹NAV: {initial_nav:,.0f}")
        print(f"     æœ€çµ‚NAV: {final_nav:,.0f}")
        print(f"     ç¸½å ±é…¬ç‡: {total_return:.4f} ({total_return*100:.2f}%)")
        print(f"     ç´¯ç©çå‹µ: {total_reward:.6f}")
        print(f"     å¹³å‡çå‹µ: {total_reward/step_count:.6f}")
        
        # åˆ†æäº¤æ˜“è¡Œç‚ºæ¨¡å¼
        actions = agent.action_history[-step_count:]  # æœ€è¿‘çš„å‹•ä½œ
        stock_selections = [action[0] for action in actions]
        position_sizes = [action[1][0] for action in actions]
        
        print(f"\n   äº¤æ˜“è¡Œç‚ºåˆ†æ:")
        print(f"     ç¸½äº¤æ˜“æ¬¡æ•¸: {len(actions)}")
        print(f"     è²·å…¥æ¬¡æ•¸: {sum(1 for pos in position_sizes if pos > 0)}")
        print(f"     è³£å‡ºæ¬¡æ•¸: {sum(1 for pos in position_sizes if pos < 0)}")
        print(f"     ç„¡æ“ä½œæ¬¡æ•¸: {sum(1 for pos in position_sizes if pos == 0)}")
        print(f"     å¹³å‡å€‰ä½å¤§å°: {np.mean(np.abs(position_sizes)):.1f}")
        
        return True, total_return, agent.performance_history
        
    except Exception as e:
        print(f"âŒ äº¤æ˜“ç­–ç•¥æ¸¬è©¦å¤±æ•—: {str(e)}")
        traceback.print_exc()
        return False, None, None

def test_performance_analysis(agent, performance_history):
    """æ¸¬è©¦ 5: ç¸¾æ•ˆåˆ†æ"""
    print("\nğŸ“Š æ¸¬è©¦ 5: ç¸¾æ•ˆåˆ†æ")
    print("-" * 40)
    
    try:
        if not performance_history:
            print("âš ï¸ ç„¡ç¸¾æ•ˆæ­·å²è³‡æ–™")
            return True
        
        # æå–ç¸¾æ•ˆæ•¸æ“š
        rewards = [p['reward'] for p in performance_history]
        navs = [p['nav'] for p in performance_history]
        position_counts = [p['positions'] for p in performance_history]
        
        print(f"   ç¸¾æ•ˆçµ±è¨ˆ:")
        print(f"     ç¸½æ­¥æ•¸: {len(rewards)}")
        print(f"     çå‹µçµ±è¨ˆ: å¹³å‡={np.mean(rewards):.6f}, æ¨™æº–å·®={np.std(rewards):.6f}")
        print(f"     NAVç¯„åœ: {min(navs):,.0f} - {max(navs):,.0f}")
        print(f"     å¹³å‡æŒå€‰æ•¸: {np.mean(position_counts):.1f}")
        
        # è¨ˆç®—é¢¨éšªæŒ‡æ¨™
        if len(rewards) > 1:
            # å¤æ™®æ¯”ç‡ (ç°¡åŒ–ç‰ˆ)
            if np.std(rewards) > 0:
                sharpe_ratio = np.mean(rewards) / np.std(rewards)
                print(f"     ç°¡åŒ–å¤æ™®æ¯”ç‡: {sharpe_ratio:.4f}")
            
            # æœ€å¤§å›æ’¤
            peak_nav = navs[0]
            max_drawdown = 0
            for nav in navs:
                if nav > peak_nav:
                    peak_nav = nav
                drawdown = (peak_nav - nav) / peak_nav
                max_drawdown = max(max_drawdown, drawdown)
            
            print(f"     æœ€å¤§å›æ’¤: {max_drawdown:.4f} ({max_drawdown*100:.2f}%)")
        
        # åˆ†æäº¤æ˜“é »ç‡
        non_zero_actions = sum(1 for action in agent.action_history 
                              if action[1][0] != 0)
        total_actions = len(agent.action_history)
        trading_frequency = non_zero_actions / total_actions if total_actions > 0 else 0
        
        print(f"     äº¤æ˜“é »ç‡: {trading_frequency:.2f} ({non_zero_actions}/{total_actions})")
        
        print("âœ… ç¸¾æ•ˆåˆ†æå®Œæˆ")
        
        return True
        
    except Exception as e:
        print(f"âŒ ç¸¾æ•ˆåˆ†æå¤±æ•—: {str(e)}")
        traceback.print_exc()
        return False

def test_edge_cases(agent, model_config):
    """æ¸¬è©¦ 6: é‚Šç•Œæƒ…æ³è™•ç†"""
    print("\nğŸ” æ¸¬è©¦ 6: é‚Šç•Œæƒ…æ³è™•ç†")
    print("-" * 40)
    
    try:
        # æ¸¬è©¦æ¥µç«¯è§€æ¸¬å€¼
        edge_cases = [
            {
                'name': 'å…¨é›¶è§€æ¸¬',
                'observation': {
                    'price_frame': torch.zeros(1, model_config.n_stocks, 64, model_config.price_frame_shape[2]),
                    'fundamental': torch.zeros(1, model_config.fundamental_dim),
                    'account': torch.zeros(1, model_config.account_dim)
                }
            },
            {
                'name': 'æ¥µå¤§å€¼è§€æ¸¬',
                'observation': {
                    'price_frame': torch.ones(1, model_config.n_stocks, 64, model_config.price_frame_shape[2]) * 1000,
                    'fundamental': torch.ones(1, model_config.fundamental_dim) * 100,
                    'account': torch.ones(1, model_config.account_dim)
                }
            },
            {
                'name': 'éš¨æ©Ÿå™ªè²è§€æ¸¬',
                'observation': {
                    'price_frame': torch.randn(1, model_config.n_stocks, 64, model_config.price_frame_shape[2]) * 10,
                    'fundamental': torch.randn(1, model_config.fundamental_dim) * 5,
                    'account': torch.randn(1, model_config.account_dim)
                }
            }
        ]
        
        for case in edge_cases:
            print(f"\n   æ¸¬è©¦: {case['name']}")
            
            try:
                action = agent.get_action(case['observation'])
                stock_idx, position_array = action
                
                print(f"     å‹•ä½œç”ŸæˆæˆåŠŸ: è‚¡ç¥¨={stock_idx}, å€‰ä½={position_array[0]}")
                
                # æª¢æŸ¥å‹•ä½œåˆç†æ€§
                assert 0 <= stock_idx < model_config.n_stocks, f"è‚¡ç¥¨ç´¢å¼•è¶…å‡ºç¯„åœ: {stock_idx}"
                assert -model_config.max_position <= position_array[0] <= model_config.max_position, \
                    f"å€‰ä½è¶…å‡ºé™åˆ¶: {position_array[0]}"
                
                print(f"     âœ… é‚Šç•Œæƒ…æ³è™•ç†æ­£å¸¸")
                
            except Exception as e:
                print(f"     âŒ é‚Šç•Œæƒ…æ³è™•ç†å¤±æ•—: {str(e)}")
                return False
        
        print("âœ… æ‰€æœ‰é‚Šç•Œæƒ…æ³æ¸¬è©¦é€šé")
        return True
        
    except Exception as e:
        print(f"âŒ é‚Šç•Œæƒ…æ³æ¸¬è©¦å¤±æ•—: {str(e)}")
        traceback.print_exc()
        return False

def run_agent_behavior_test():
    """åŸ·è¡Œä»£ç†äººè¡Œç‚ºæ¸¬è©¦"""
    print("=" * 60)
    print("TSE Alpha ä»£ç†äººè¡Œç‚ºæ¸¬è©¦")
    print("æ¸¬è©¦æ±ºç­–é‚è¼¯ã€é¢¨éšªç®¡ç†å’Œäº¤æ˜“ç­–ç•¥è¡Œç‚º")
    print("=" * 60)
    
    tests_passed = 0
    total_tests = 6
    
    # åˆå§‹åŒ–è®Šæ•¸
    agent = None
    model_config = None
    training_config = None
    observation = None
    performance_history = None
    
    # æ¸¬è©¦ 1: ä»£ç†äººå‰µå»ºå’Œåˆå§‹åŒ–
    success, agent, model_config, training_config = test_agent_creation()
    if success:
        tests_passed += 1
    
    # æ¸¬è©¦ 2: æ±ºç­–ä¸€è‡´æ€§
    if agent and model_config:
        success, observation = test_decision_consistency(agent, model_config)
        if success:
            tests_passed += 1
    
    # æ¸¬è©¦ 3: é¢¨éšªç®¡ç†æ©Ÿåˆ¶
    if agent and observation and model_config:
        success = test_risk_management(agent, observation, model_config)
        if success:
            tests_passed += 1
    
    # æ¸¬è©¦ 4: äº¤æ˜“ç­–ç•¥è¡Œç‚º
    if agent and model_config and training_config:
        success, total_return, performance_history = test_trading_strategy(
            agent, model_config, training_config)
        if success:
            tests_passed += 1
    
    # æ¸¬è©¦ 5: ç¸¾æ•ˆåˆ†æ
    if agent and performance_history:
        success = test_performance_analysis(agent, performance_history)
        if success:
            tests_passed += 1
    
    # æ¸¬è©¦ 6: é‚Šç•Œæƒ…æ³è™•ç†
    if agent and model_config:
        success = test_edge_cases(agent, model_config)
        if success:
            tests_passed += 1
    
    # æ¸¬è©¦çµæœç¸½çµ
    print("\n" + "=" * 60)
    print("ğŸ“‹ ä»£ç†äººè¡Œç‚ºæ¸¬è©¦çµæœ")
    print("=" * 60)
    
    pass_rate = (tests_passed / total_tests) * 100
    
    print(f"ç¸½æ¸¬è©¦æ•¸: {total_tests}")
    print(f"é€šéæ¸¬è©¦: {tests_passed}")
    print(f"å¤±æ•—æ¸¬è©¦: {total_tests - tests_passed}")
    print(f"é€šéç‡: {pass_rate:.1f}%")
    
    if pass_rate >= 85:
        print(f"\nğŸ‰ ä»£ç†äººè¡Œç‚ºæ¸¬è©¦é€šéï¼")
        print(f"âœ… æ±ºç­–é‚è¼¯æ­£å¸¸é‹ä½œ")
        print(f"âœ… é¢¨éšªç®¡ç†æ©Ÿåˆ¶æœ‰æ•ˆ")
        print(f"âœ… äº¤æ˜“ç­–ç•¥è¡Œç‚ºåˆç†")
        print(f"ğŸš€ å¯ä»¥é€²è¡Œç«¯åˆ°ç«¯è¨“ç·´æ¸¬è©¦")
        
        print(f"\nğŸ¯ å»ºè­°ä¸‹ä¸€æ­¥:")
        print(f"   1. åŸ·è¡Œç«¯åˆ°ç«¯è¨“ç·´æ¸¬è©¦")
        print(f"   2. é€²è¡Œæ€§èƒ½åŸºæº–æ¸¬è©¦")
        print(f"   3. æ¸¬è©¦å›æ¸¬å¼•æ“")
        
    elif pass_rate >= 70:
        print(f"\nâœ… ä»£ç†äººåŸºæœ¬è¡Œç‚ºæ­£å¸¸")
        print(f"ğŸ”§ éƒ¨åˆ†ç­–ç•¥å¯èƒ½éœ€è¦å„ªåŒ–")
        
    else:
        print(f"\nâš ï¸ ä»£ç†äººè¡Œç‚ºå­˜åœ¨é‡è¦å•é¡Œ")
        print(f"ğŸ”§ éœ€è¦ä¿®å¾©å¤±æ•—çš„æ¸¬è©¦é …ç›®")
    
    return pass_rate >= 70

if __name__ == "__main__":
    success = run_agent_behavior_test()
    print(f"\n{'âœ… æ¸¬è©¦é€šé' if success else 'âŒ æ¸¬è©¦å¤±æ•—'}")
    sys.exit(0 if success else 1)
#!/usr/bin/env python3
"""
RTX 4090 éšæ®µ1åŸ·è¡Œè…³æœ¬
è‡ªå‹•åŒ–åŸ·è¡Œè¨˜æ†¶é«”ä¿®å¾©å’ŒBæ–¹æ¡ˆé©—è­‰
"""

import os
import sys
import subprocess
import torch
import time
from pathlib import Path

def print_header(title):
    """æ‰“å°æ¨™é¡Œ"""
    print("\n" + "="*60)
    print(f"ğŸš€ {title}")
    print("="*60)

def print_step(step_num, description):
    """æ‰“å°æ­¥é©Ÿ"""
    print(f"\nğŸ“‹ æ­¥é©Ÿ {step_num}: {description}")
    print("-" * 40)

def check_pytorch_version():
    """æª¢æŸ¥PyTorchç‰ˆæœ¬ä¸¦è¨­ç½®è¨˜æ†¶é«”ç®¡ç†"""
    print_step(1, "æª¢æŸ¥PyTorchç‰ˆæœ¬ä¸¦è¨­ç½®è¨˜æ†¶é«”ç®¡ç†")
    
    try:
        print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
        print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
        
        if torch.cuda.is_available():
            print(f"GPU: {torch.cuda.get_device_name(0)}")
            print(f"GPUè¨˜æ†¶é«”: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        else:
            print("âŒ æœªæª¢æ¸¬åˆ°CUDA GPU")
            return False
        
        # æª¢æŸ¥PyTorchç‰ˆæœ¬ä¸¦è¨­ç½®è¨˜æ†¶é«”ç®¡ç†
        pytorch_version = torch.__version__
        major, minor = map(int, pytorch_version.split('.')[:2])
        
        if major > 2 or (major == 2 and minor >= 3):
            print("âœ… PyTorch â‰¥ 2.3ï¼Œä½¿ç”¨ expandable_segments")
            os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
        elif major == 2 and minor == 2:
            print("âœ… PyTorch 2.2ï¼Œä½¿ç”¨ max_split_size_mb")
            os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:64,garbage_collection_threshold:0.6'
        else:
            print("âš ï¸ PyTorchç‰ˆæœ¬è¼ƒèˆŠï¼Œä½¿ç”¨é è¨­è¨­ç½®")
        
        # æ¸…ç†GPUè¨˜æ†¶é«”
        print("ğŸ§¹ æ¸…ç†GPUè¨˜æ†¶é«”...")
        torch.cuda.empty_cache()
        
        return True
        
    except Exception as e:
        print(f"âŒ PyTorchæª¢æŸ¥å¤±æ•—: {e}")
        return False

def verify_config_file():
    """é©—è­‰é…ç½®æ–‡ä»¶å­˜åœ¨"""
    print_step(2, "é©—è­‰RTX 4090æ¸¬è©¦é…ç½®æ–‡ä»¶")
    
    config_file = "rtx4090_optimized_config.yaml"
    
    if os.path.exists(config_file):
        print(f"âœ… æ‰¾åˆ°é…ç½®æ–‡ä»¶: {config_file}")
        
        # é¡¯ç¤ºé—œéµé…ç½®
        try:
            import yaml
            with open(config_file, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            
            print("ğŸ“Š é—œéµé…ç½®:")
            print(f"  - è‚¡ç¥¨æ•¸é‡: {config['data']['stocks']['total']}")
            print(f"  - æ‰¹æ¬¡å¤§å°: {config['data']['loading']['batch_size']}")
            print(f"  - d_model: {config['model']['transformer']['d_model']}")
            print(f"  - n_layers: {config['model']['transformer']['n_layers']}")
            print(f"  - è¨“ç·´è¼ªæ•¸: {config['training']['max_epochs']}")
            
        except Exception as e:
            print(f"âš ï¸ ç„¡æ³•è§£æé…ç½®æ–‡ä»¶: {e}")
        
        return True
    else:
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
        return False

def run_smoke_test():
    """é‹è¡Œç…™éœ§æ¸¬è©¦"""
    print_step(3, "åŸ·è¡ŒBæ–¹æ¡ˆé©—è­‰ç…™éœ§æ¸¬è©¦")
    
    try:
        # æª¢æŸ¥step0_quick_validation.pyæ˜¯å¦å­˜åœ¨
        if not os.path.exists("step0_quick_validation.py"):
            print("âŒ step0_quick_validation.py ä¸å­˜åœ¨")
            return False
        
        print("ğŸ”¥ é–‹å§‹ç…™éœ§æ¸¬è©¦...")
        print("â±ï¸ é è¨ˆéœ€è¦5-10åˆ†é˜...")
        
        # æ§‹å»ºå‘½ä»¤
        cmd = [
            sys.executable, 
            "step0_quick_validation.py", 
            "--smoke", 
            "--config", 
            "rtx4090_optimized_config.yaml"
        ]
        
        print(f"åŸ·è¡Œå‘½ä»¤: {' '.join(cmd)}")
        
        # åŸ·è¡Œæ¸¬è©¦
        start_time = time.time()
        result = subprocess.run(
            cmd, 
            capture_output=True, 
            text=True, 
            timeout=1800  # 30åˆ†é˜è¶…æ™‚
        )
        
        end_time = time.time()
        duration = end_time - start_time
        
        print(f"â±ï¸ åŸ·è¡Œæ™‚é–“: {duration:.1f} ç§’")
        
        # æª¢æŸ¥çµæœ
        if result.returncode == 0:
            print("âœ… ç…™éœ§æ¸¬è©¦æˆåŠŸ!")
            print("\nğŸ“Š æ¸¬è©¦è¼¸å‡º:")
            print(result.stdout)
            return True
        else:
            print("âŒ ç…™éœ§æ¸¬è©¦å¤±æ•—!")
            print("\nğŸ“Š éŒ¯èª¤è¼¸å‡º:")
            print(result.stderr)
            print("\nğŸ“Š æ¨™æº–è¼¸å‡º:")
            print(result.stdout)
            return False
            
    except subprocess.TimeoutExpired:
        print("âŒ ç…™éœ§æ¸¬è©¦è¶…æ™‚ (30åˆ†é˜)")
        return False
    except Exception as e:
        print(f"âŒ ç…™éœ§æ¸¬è©¦åŸ·è¡Œå¤±æ•—: {e}")
        return False

def check_gpu_memory():
    """æª¢æŸ¥GPUè¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³"""
    print_step(4, "æª¢æŸ¥GPUè¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³")
    
    try:
        if torch.cuda.is_available():
            # ç²å–è¨˜æ†¶é«”ä¿¡æ¯
            total_memory = torch.cuda.get_device_properties(0).total_memory
            allocated_memory = torch.cuda.memory_allocated(0)
            reserved_memory = torch.cuda.memory_reserved(0)
            
            print(f"ğŸ“Š GPUè¨˜æ†¶é«”ç‹€æ³:")
            print(f"  - ç¸½è¨˜æ†¶é«”: {total_memory / 1024**3:.2f} GB")
            print(f"  - å·²åˆ†é…: {allocated_memory / 1024**3:.2f} GB ({allocated_memory/total_memory*100:.1f}%)")
            print(f"  - å·²ä¿ç•™: {reserved_memory / 1024**3:.2f} GB ({reserved_memory/total_memory*100:.1f}%)")
            print(f"  - å¯ç”¨: {(total_memory - reserved_memory) / 1024**3:.2f} GB")
            
            # æª¢æŸ¥æ˜¯å¦æœ‰è¨˜æ†¶é«”æ´©æ¼
            if allocated_memory > 0:
                print("âš ï¸ æª¢æ¸¬åˆ°GPUè¨˜æ†¶é«”ä½¿ç”¨ï¼Œæ¸…ç†ä¸­...")
                torch.cuda.empty_cache()
                
                # å†æ¬¡æª¢æŸ¥
                allocated_after = torch.cuda.memory_allocated(0)
                reserved_after = torch.cuda.memory_reserved(0)
                
                print(f"ğŸ§¹ æ¸…ç†å¾Œ:")
                print(f"  - å·²åˆ†é…: {allocated_after / 1024**3:.2f} GB")
                print(f"  - å·²ä¿ç•™: {reserved_after / 1024**3:.2f} GB")
            
            return True
        else:
            print("âŒ ç„¡GPUå¯ç”¨")
            return False
            
    except Exception as e:
        print(f"âŒ GPUè¨˜æ†¶é«”æª¢æŸ¥å¤±æ•—: {e}")
        return False

def generate_next_steps():
    """ç”Ÿæˆä¸‹ä¸€æ­¥å»ºè­°"""
    print_step(5, "ç”Ÿæˆä¸‹ä¸€æ­¥åŸ·è¡Œå»ºè­°")
    
    print("ğŸ¯ éšæ®µ1å®Œæˆ! ä¸‹ä¸€æ­¥å»ºè­°:")
    print("\nğŸ“‹ å¦‚æœæ¸¬è©¦æˆåŠŸ:")
    print("  1. é€²å…¥éšæ®µ2: RTX 4090åŸºç¤å„ªåŒ–")
    print("     - DataLoaderå„ªåŒ– (+7~10%)")
    print("     - GPUè¨ˆç®—å„ªåŒ– (+20-35%)")
    print("     - fused AdamWå„ªåŒ–å™¨")
    print("\nğŸ“‹ åŸ·è¡Œå‘½ä»¤:")
    print("  python step0_quick_validation.py --smoke --compile-mode reduce-overhead")
    
    print("\nğŸ“‹ å¦‚æœæ¸¬è©¦å¤±æ•—:")
    print("  1. æª¢æŸ¥éŒ¯èª¤æ—¥èªŒ")
    print("  2. é€²ä¸€æ­¥èª¿æ•´æ‰¹æ¬¡å¤§å°")
    print("  3. æ¸›å°‘æ¨¡å‹åƒæ•¸")
    print("  4. æª¢æŸ¥ç¡¬é«”é…ç½®")
    
    print("\nğŸ“‹ ç›£æ§æŒ‡æ¨™:")
    print("  - GPUåˆ©ç”¨ç‡æ‡‰ >80%")
    print("  - è¨˜æ†¶é«”ä½¿ç”¨æ‡‰ <90%")
    print("  - è¨“ç·´é€Ÿåº¦æ‡‰ >50 samples/sec")

def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    print_header("RTX 4090 éšæ®µ1åŸ·è¡Œ - è¨˜æ†¶é«”ä¿®å¾©èˆ‡Bæ–¹æ¡ˆé©—è­‰")
    
    print("ğŸ¯ ç›®æ¨™:")
    print("  - è§£æ±ºGPUè¨˜æ†¶é«”ä¸è¶³å•é¡Œ")
    print("  - é©—è­‰Bæ–¹æ¡ˆä¿®å¾© (27å€‹ç‰¹å¾µ)")
    print("  - ç¢ºä¿ç³»çµ±å¯ä»¥æ­£å¸¸é‹è¡Œ")
    
    # åŸ·è¡Œå„å€‹æ­¥é©Ÿ
    steps = [
        ("PyTorchç‰ˆæœ¬æª¢æŸ¥", check_pytorch_version),
        ("é…ç½®æ–‡ä»¶é©—è­‰", verify_config_file),
        ("ç…™éœ§æ¸¬è©¦åŸ·è¡Œ", run_smoke_test),
        ("GPUè¨˜æ†¶é«”æª¢æŸ¥", check_gpu_memory),
        ("ä¸‹ä¸€æ­¥å»ºè­°", generate_next_steps)
    ]
    
    success_count = 0
    total_steps = len(steps) - 1  # æœ€å¾Œä¸€æ­¥æ˜¯å»ºè­°ï¼Œä¸è¨ˆå…¥æˆåŠŸç‡
    
    for step_name, step_func in steps:
        try:
            if step_func():
                if step_name != "ä¸‹ä¸€æ­¥å»ºè­°":  # å»ºè­°æ­¥é©Ÿç¸½æ˜¯åŸ·è¡Œ
                    success_count += 1
                    print(f"âœ… {step_name} æˆåŠŸ")
            else:
                print(f"âŒ {step_name} å¤±æ•—")
                if step_name == "ç…™éœ§æ¸¬è©¦åŸ·è¡Œ":
                    print("\nâš ï¸ ç…™éœ§æ¸¬è©¦å¤±æ•—ï¼Œä½†ç¹¼çºŒåŸ·è¡Œå¾ŒçºŒæª¢æŸ¥...")
        except Exception as e:
            print(f"âŒ {step_name} ç•°å¸¸: {e}")
    
    # ç¸½çµ
    print_header("éšæ®µ1åŸ·è¡Œç¸½çµ")
    
    success_rate = (success_count / total_steps) * 100
    print(f"ğŸ“Š æˆåŠŸç‡: {success_count}/{total_steps} ({success_rate:.1f}%)")
    
    if success_rate >= 75:
        print("ğŸ‰ éšæ®µ1åŸºæœ¬æˆåŠŸ! å¯ä»¥é€²å…¥éšæ®µ2")
        print("ğŸš€ å»ºè­°ç«‹å³åŸ·è¡Œéšæ®µ2åŸºç¤å„ªåŒ–")
    elif success_rate >= 50:
        print("âš ï¸ éšæ®µ1éƒ¨åˆ†æˆåŠŸï¼Œéœ€è¦èª¿æ•´å¾Œé‡è©¦")
        print("ğŸ”§ å»ºè­°æª¢æŸ¥å¤±æ•—æ­¥é©Ÿä¸¦é€²è¡Œä¿®å¾©")
    else:
        print("âŒ éšæ®µ1å¤±æ•—ï¼Œéœ€è¦æ·±å…¥è¨ºæ–·")
        print("ğŸ” å»ºè­°æª¢æŸ¥ç¡¬é«”é…ç½®å’Œç’°å¢ƒè¨­ç½®")
    
    print(f"\nğŸ“ è©³ç´°æ—¥èªŒå·²ä¿å­˜åˆ°: logs/rtx4090_test.log")
    print(f"ğŸ“Š TensorBoardæ—¥èªŒ: logs/tensorboard_rtx4090_test")

if __name__ == "__main__":
    main()
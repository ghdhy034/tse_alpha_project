#!/usr/bin/env python3
"""
TSE Alpha ç«¯åˆ°ç«¯è¨“ç·´æ¸¬è©¦è…³æœ¬
æ¸¬è©¦å®Œæ•´çš„è¨“ç·´æµç¨‹ï¼Œå¾è³‡æ–™è¼‰å…¥åˆ°æ¨¡å‹è¨“ç·´
"""

import sys
import os
from pathlib import Path
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from typing import Dict, Any, List, Tuple
import traceback
import time

# æ·»åŠ è·¯å¾‘
sys.path.append(str(Path(__file__).parent))

def test_data_loading():
    """æ¸¬è©¦ 1: è³‡æ–™è¼‰å…¥å’Œé è™•ç†"""
    print("ğŸ“Š æ¸¬è©¦ 1: è³‡æ–™è¼‰å…¥å’Œé è™•ç†")
    print("-" * 40)
    
    try:
        from models.config.training_config import TrainingConfig
        from models.data_loader import TSEAlphaDataLoader
        
        # å‰µå»ºé…ç½®
        config = TrainingConfig()
        
        print("âœ… è¨“ç·´é…ç½®è¼‰å…¥æˆåŠŸ")
        print(f"   åºåˆ—é•·åº¦: {config.sequence_length}")
        print(f"   åƒ¹æ ¼ç‰¹å¾µæ•¸: {config.price_features}")
        print(f"   åŸºæœ¬é¢ç‰¹å¾µæ•¸: {config.fundamental_features}")
        
        # å‰µå»ºè³‡æ–™è¼‰å…¥å™¨
        data_loader = TSEAlphaDataLoader(config)
        
        print("âœ… è³‡æ–™è¼‰å…¥å™¨å‰µå»ºæˆåŠŸ")
        
        # æ¸¬è©¦å°è¦æ¨¡è³‡æ–™è¼‰å…¥
        test_symbols = ['2330', '2317', '2454']
        start_date = '2024-01-01'
        end_date = '2024-01-31'
        
        print(f"\n   è¼‰å…¥æ¸¬è©¦è³‡æ–™: {test_symbols}")
        print(f"   æ™‚é–“ç¯„åœ: {start_date} ~ {end_date}")
        
        # è¼‰å…¥è³‡æ–™
        dataset = data_loader.load_data(
            symbols=test_symbols,
            start_date=start_date,
            end_date=end_date,
            split='train'
        )
        
        print("âœ… è³‡æ–™è¼‰å…¥æˆåŠŸ")
        print(f"   è³‡æ–™é›†å¤§å°: {len(dataset)}")
        
        # æª¢æŸ¥è³‡æ–™æ ¼å¼
        if len(dataset) > 0:
            sample = dataset[0]
            print(f"\n   æ¨£æœ¬æ ¼å¼æª¢æŸ¥:")
            for key, value in sample.items():
                if isinstance(value, torch.Tensor):
                    print(f"     {key}: {value.shape} ({value.dtype})")
                else:
                    print(f"     {key}: {type(value)} = {value}")
        
        return True, data_loader, dataset, config
        
    except Exception as e:
        print(f"âŒ è³‡æ–™è¼‰å…¥æ¸¬è©¦å¤±æ•—: {str(e)}")
        traceback.print_exc()
        return False, None, None, None

def test_model_setup(config):
    """æ¸¬è©¦ 2: æ¨¡å‹è¨­ç½®å’Œåˆå§‹åŒ–"""
    print("\nğŸ¤– æ¸¬è©¦ 2: æ¨¡å‹è¨­ç½®å’Œåˆå§‹åŒ–")
    print("-" * 40)
    
    try:
        from models.model_architecture import ModelConfig, TSEAlphaModel
        
        # å‰µå»ºæ¨¡å‹é…ç½®
        model_config = ModelConfig(
            price_frame_shape=(3, config.sequence_length, config.price_features),
            fundamental_dim=config.fundamental_features,
            account_dim=config.account_features,
            n_stocks=3,  # æ¸¬è©¦ç”¨è¼ƒå°‘è‚¡ç¥¨
            hidden_dim=64  # è¼ƒå°çš„éš±è—ç¶­åº¦
        )
        
        # å‰µå»ºæ¨¡å‹
        model = TSEAlphaModel(model_config)
        
        print("âœ… æ¨¡å‹å‰µå»ºæˆåŠŸ")
        
        # è¨ˆç®—æ¨¡å‹å¤§å°
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        print(f"   ç¸½åƒæ•¸æ•¸: {total_params:,}")
        print(f"   å¯è¨“ç·´åƒæ•¸: {trainable_params:,}")
        print(f"   æ¨¡å‹å¤§å°: {total_params * 4 / 1024 / 1024:.2f} MB")
        
        # æ¸¬è©¦æ¨¡å‹å‰å‘å‚³æ’­
        batch_size = 2
        test_input = {
            'price_frame': torch.randn(batch_size, 3, config.sequence_length, config.price_features),
            'fundamental': torch.randn(batch_size, config.fundamental_features),
            'account': torch.randn(batch_size, config.account_features)
        }
        
        model.eval()
        with torch.no_grad():
            outputs = model(test_input)
        
        print("âœ… æ¨¡å‹å‰å‘å‚³æ’­æˆåŠŸ")
        print(f"   è¼¸å‡ºå½¢ç‹€:")
        for key, value in outputs.items():
            if isinstance(value, torch.Tensor):
                print(f"     {key}: {value.shape}")
        
        return True, model, model_config
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹è¨­ç½®æ¸¬è©¦å¤±æ•—: {str(e)}")
        traceback.print_exc()
        return False, None, None

def create_synthetic_dataset(config, n_samples=100):
    """å‰µå»ºåˆæˆè³‡æ–™é›†ç”¨æ–¼è¨“ç·´æ¸¬è©¦"""
    print("\n   å‰µå»ºåˆæˆè³‡æ–™é›†...")
    
    # å‰µå»ºåˆæˆè³‡æ–™
    price_frames = torch.randn(n_samples, 3, config.sequence_length, config.price_features)
    fundamentals = torch.randn(n_samples, config.fundamental_features)
    accounts = torch.randn(n_samples, config.account_features)
    
    # å‰µå»ºåˆæˆæ¨™ç±¤ (è‚¡ç¥¨é¸æ“‡ + å€‰ä½å¤§å°)
    stock_labels = torch.randint(0, 3, (n_samples,))  # 3æª”è‚¡ç¥¨
    position_labels = torch.randn(n_samples, 1) * 100  # å€‰ä½å¤§å°
    
    # å‰µå»ºè³‡æ–™é›†
    dataset = TensorDataset(
        price_frames, fundamentals, accounts, 
        stock_labels, position_labels
    )
    
    print(f"   åˆæˆè³‡æ–™é›†å¤§å°: {len(dataset)}")
    return dataset

def test_training_loop(model, config):
    """æ¸¬è©¦ 3: è¨“ç·´å¾ªç’°"""
    print("\nğŸ‹ï¸ æ¸¬è©¦ 3: è¨“ç·´å¾ªç’°")
    print("-" * 40)
    
    try:
        # å‰µå»ºåˆæˆè³‡æ–™é›†
        train_dataset = create_synthetic_dataset(config, n_samples=50)
        val_dataset = create_synthetic_dataset(config, n_samples=20)
        
        # å‰µå»ºè³‡æ–™è¼‰å…¥å™¨
        train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)
        
        print("âœ… è³‡æ–™è¼‰å…¥å™¨å‰µå»ºæˆåŠŸ")
        
        # è¨­ç½®å„ªåŒ–å™¨å’Œæå¤±å‡½æ•¸
        optimizer = optim.Adam(model.parameters(), lr=1e-4)
        
        # å¤šä»»å‹™æå¤±å‡½æ•¸
        stock_criterion = nn.CrossEntropyLoss()
        position_criterion = nn.MSELoss()
        value_criterion = nn.MSELoss()
        
        print("âœ… å„ªåŒ–å™¨å’Œæå¤±å‡½æ•¸è¨­ç½®æˆåŠŸ")
        
        # è¨“ç·´å¾ªç’°
        model.train()
        num_epochs = 3  # å°‘é‡ epoch ç”¨æ–¼æ¸¬è©¦
        
        print(f"\n   é–‹å§‹è¨“ç·´ ({num_epochs} epochs)...")
        
        training_history = []
        
        for epoch in range(num_epochs):
            epoch_start_time = time.time()
            
            # è¨“ç·´éšæ®µ
            train_loss = 0.0
            train_batches = 0
            
            for batch_idx, (price_frames, fundamentals, accounts, stock_labels, position_labels) in enumerate(train_loader):
                optimizer.zero_grad()
                
                # æº–å‚™è¼¸å…¥
                observation = {
                    'price_frame': price_frames,
                    'fundamental': fundamentals,
                    'account': accounts
                }
                
                # å‰å‘å‚³æ’­
                outputs = model(observation)
                
                # è¨ˆç®—æå¤±
                stock_loss = stock_criterion(outputs['stock_logits'], stock_labels)
                position_loss = position_criterion(outputs['position_size'], position_labels)
                value_loss = value_criterion(outputs['value'], torch.zeros_like(outputs['value']))
                
                total_loss = stock_loss + position_loss + 0.1 * value_loss
                
                # åå‘å‚³æ’­
                total_loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                
                optimizer.step()
                
                train_loss += total_loss.item()
                train_batches += 1
                
                if batch_idx == 0:  # åªé¡¯ç¤ºç¬¬ä¸€å€‹æ‰¹æ¬¡çš„è©³ç´°ä¿¡æ¯
                    print(f"     Epoch {epoch+1}, Batch {batch_idx+1}:")
                    print(f"       è‚¡ç¥¨é¸æ“‡æå¤±: {stock_loss.item():.4f}")
                    print(f"       å€‰ä½å¤§å°æå¤±: {position_loss.item():.4f}")
                    print(f"       åƒ¹å€¼ä¼°è¨ˆæå¤±: {value_loss.item():.4f}")
                    print(f"       ç¸½æå¤±: {total_loss.item():.4f}")
            
            avg_train_loss = train_loss / train_batches
            
            # é©—è­‰éšæ®µ
            model.eval()
            val_loss = 0.0
            val_batches = 0
            
            with torch.no_grad():
                for price_frames, fundamentals, accounts, stock_labels, position_labels in val_loader:
                    observation = {
                        'price_frame': price_frames,
                        'fundamental': fundamentals,
                        'account': accounts
                    }
                    
                    outputs = model(observation)
                    
                    stock_loss = stock_criterion(outputs['stock_logits'], stock_labels)
                    position_loss = position_criterion(outputs['position_size'], position_labels)
                    value_loss = value_criterion(outputs['value'], torch.zeros_like(outputs['value']))
                    
                    total_loss = stock_loss + position_loss + 0.1 * value_loss
                    val_loss += total_loss.item()
                    val_batches += 1
            
            avg_val_loss = val_loss / val_batches
            epoch_time = time.time() - epoch_start_time
            
            print(f"   Epoch {epoch+1}/{num_epochs}:")
            print(f"     è¨“ç·´æå¤±: {avg_train_loss:.4f}")
            print(f"     é©—è­‰æå¤±: {avg_val_loss:.4f}")
            print(f"     è€—æ™‚: {epoch_time:.2f}s")
            
            training_history.append({
                'epoch': epoch + 1,
                'train_loss': avg_train_loss,
                'val_loss': avg_val_loss,
                'time': epoch_time
            })
            
            model.train()
        
        print("âœ… è¨“ç·´å¾ªç’°å®Œæˆ")
        
        # åˆ†æè¨“ç·´çµæœ
        initial_loss = training_history[0]['train_loss']
        final_loss = training_history[-1]['train_loss']
        loss_reduction = (initial_loss - final_loss) / initial_loss
        
        print(f"\n   è¨“ç·´çµæœåˆ†æ:")
        print(f"     åˆå§‹æå¤±: {initial_loss:.4f}")
        print(f"     æœ€çµ‚æå¤±: {final_loss:.4f}")
        print(f"     æå¤±ä¸‹é™: {loss_reduction:.2%}")
        print(f"     å¹³å‡æ¯epochè€—æ™‚: {np.mean([h['time'] for h in training_history]):.2f}s")
        
        return True, training_history
        
    except Exception as e:
        print(f"âŒ è¨“ç·´å¾ªç’°æ¸¬è©¦å¤±æ•—: {str(e)}")
        traceback.print_exc()
        return False, None

def test_model_saving_loading(model, config):
    """æ¸¬è©¦ 4: æ¨¡å‹ä¿å­˜å’Œè¼‰å…¥"""
    print("\nğŸ’¾ æ¸¬è©¦ 4: æ¨¡å‹ä¿å­˜å’Œè¼‰å…¥")
    print("-" * 40)
    
    try:
        # ä¿å­˜æ¨¡å‹
        save_path = "tmp_test_model.pth"
        
        # ä¿å­˜æ¨¡å‹ç‹€æ…‹
        torch.save({
            'model_state_dict': model.state_dict(),
            'config': config.to_dict(),
            'model_config': {
                'price_frame_shape': (3, config.sequence_length, config.price_features),
                'fundamental_dim': config.fundamental_features,
                'account_dim': config.account_features,
                'n_stocks': 3,
                'hidden_dim': 64
            }
        }, save_path)
        
        print("âœ… æ¨¡å‹ä¿å­˜æˆåŠŸ")
        
        # è¼‰å…¥æ¨¡å‹
        from models.model_architecture import ModelConfig, TSEAlphaModel
        
        checkpoint = torch.load(save_path, map_location='cpu')
        
        # é‡å»ºæ¨¡å‹é…ç½®
        model_config_dict = checkpoint['model_config']
        model_config = ModelConfig(**model_config_dict)
        
        # å‰µå»ºæ–°æ¨¡å‹ä¸¦è¼‰å…¥æ¬Šé‡
        new_model = TSEAlphaModel(model_config)
        new_model.load_state_dict(checkpoint['model_state_dict'])
        
        print("âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ")
        
        # é©—è­‰æ¨¡å‹ä¸€è‡´æ€§
        test_input = {
            'price_frame': torch.randn(1, 3, config.sequence_length, config.price_features),
            'fundamental': torch.randn(1, config.fundamental_features),
            'account': torch.randn(1, config.account_features)
        }
        
        model.eval()
        new_model.eval()
        
        with torch.no_grad():
            original_output = model(test_input)
            loaded_output = new_model(test_input)
        
        # æª¢æŸ¥è¼¸å‡ºä¸€è‡´æ€§
        for key in original_output.keys():
            if isinstance(original_output[key], torch.Tensor):
                diff = torch.abs(original_output[key] - loaded_output[key]).max().item()
                print(f"   {key} æœ€å¤§å·®ç•°: {diff:.8f}")
                assert diff < 1e-6, f"{key} è¼¸å‡ºä¸ä¸€è‡´"
        
        print("âœ… æ¨¡å‹ä¸€è‡´æ€§é©—è­‰é€šé")
        
        # æ¸…ç†æ¸¬è©¦æ–‡ä»¶
        os.remove(save_path)
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹ä¿å­˜è¼‰å…¥æ¸¬è©¦å¤±æ•—: {str(e)}")
        traceback.print_exc()
        return False

def test_environment_integration(model, config):
    """æ¸¬è©¦ 5: ç’°å¢ƒæ•´åˆæ¸¬è©¦"""
    print("\nğŸŒ æ¸¬è©¦ 5: ç’°å¢ƒæ•´åˆæ¸¬è©¦")
    print("-" * 40)
    
    try:
        from gym_env.env import TSEAlphaEnv
        
        # å‰µå»ºç’°å¢ƒ
        env = TSEAlphaEnv(
            symbols=['2330', '2317', '2454'],
            start_date='2024-01-01',
            end_date='2024-01-10',
            initial_cash=1000000.0
        )
        
        # é‡ç½®ç’°å¢ƒ
        observation, info = env.reset(seed=42)
        
        print("âœ… ç’°å¢ƒå‰µå»ºå’Œé‡ç½®æˆåŠŸ")
        
        # æ¸¬è©¦æ¨¡å‹èˆ‡ç’°å¢ƒçš„æ•´åˆ
        model.eval()
        total_reward = 0.0
        step_count = 0
        max_steps = 5
        
        print(f"\n   åŸ·è¡Œ {max_steps} æ­¥æ•´åˆæ¸¬è©¦...")
        
        for step in range(max_steps):
            # èª¿æ•´è§€æ¸¬æ ¼å¼çµ¦æ¨¡å‹
            env_price_frame = observation['price_frame']
            n_stocks = 3  # æ¨¡å‹æœŸæœ›çš„è‚¡ç¥¨æ•¸
            
            if env_price_frame.shape[0] != n_stocks:
                if env_price_frame.shape[0] < n_stocks:
                    padding = np.zeros((n_stocks - env_price_frame.shape[0], 64, 5), dtype=np.float32)
                    adjusted_price_frame = np.concatenate([env_price_frame, padding], axis=0)
                else:
                    adjusted_price_frame = env_price_frame[:n_stocks]
            else:
                adjusted_price_frame = env_price_frame
            
            model_observation = {
                'price_frame': torch.tensor(adjusted_price_frame).unsqueeze(0),
                'fundamental': torch.tensor(observation['fundamental']).unsqueeze(0),
                'account': torch.tensor(observation['account']).unsqueeze(0)
            }
            
            # æ¨¡å‹ç”Ÿæˆå‹•ä½œ
            with torch.no_grad():
                action = model.get_action(model_observation, deterministic=True)
            
            # èª¿æ•´å‹•ä½œçµ¦ç’°å¢ƒ
            stock_idx, position_array = action
            env_n_stocks = len(env.symbols)
            if stock_idx >= env_n_stocks:
                stock_idx = stock_idx % env_n_stocks
            
            adjusted_action = (stock_idx, position_array)
            
            # ç’°å¢ƒåŸ·è¡Œå‹•ä½œ
            observation, reward, terminated, truncated, info = env.step(adjusted_action)
            
            total_reward += reward
            step_count += 1
            
            print(f"     æ­¥é©Ÿ {step+1}: å‹•ä½œ=({stock_idx}, {position_array[0]}), "
                  f"çå‹µ={reward:.4f}, NAV={info.get('nav', 0):,.0f}")
            
            if terminated or truncated:
                break
        
        print("âœ… ç’°å¢ƒæ•´åˆæ¸¬è©¦å®Œæˆ")
        print(f"   ç¸½æ­¥æ•¸: {step_count}")
        print(f"   ç´¯ç©çå‹µ: {total_reward:.6f}")
        print(f"   æœ€çµ‚NAV: {info.get('nav', 0):,.0f}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ç’°å¢ƒæ•´åˆæ¸¬è©¦å¤±æ•—: {str(e)}")
        traceback.print_exc()
        return False

def test_performance_benchmark(model, config):
    """æ¸¬è©¦ 6: æ€§èƒ½åŸºæº–æ¸¬è©¦"""
    print("\nâš¡ æ¸¬è©¦ 6: æ€§èƒ½åŸºæº–æ¸¬è©¦")
    print("-" * 40)
    
    try:
        # æº–å‚™æ¸¬è©¦è³‡æ–™
        batch_sizes = [1, 4, 8]
        n_stocks = 3
        
        print("   æ¸¬è©¦ä¸åŒæ‰¹æ¬¡å¤§å°çš„æ€§èƒ½...")
        
        for batch_size in batch_sizes:
            test_input = {
                'price_frame': torch.randn(batch_size, n_stocks, config.sequence_length, config.price_features),
                'fundamental': torch.randn(batch_size, config.fundamental_features),
                'account': torch.randn(batch_size, config.account_features)
            }
            
            model.eval()
            
            # é ç†±
            with torch.no_grad():
                for _ in range(5):
                    _ = model(test_input)
            
            # æ€§èƒ½æ¸¬è©¦
            start_time = time.time()
            n_iterations = 100
            
            with torch.no_grad():
                for _ in range(n_iterations):
                    outputs = model(test_input)
            
            end_time = time.time()
            
            total_time = end_time - start_time
            avg_time = total_time / n_iterations
            throughput = batch_size / avg_time
            
            print(f"     æ‰¹æ¬¡å¤§å° {batch_size}:")
            print(f"       å¹³å‡æ¨ç†æ™‚é–“: {avg_time*1000:.2f}ms")
            print(f"       ååé‡: {throughput:.1f} æ¨£æœ¬/ç§’")
        
        # è¨˜æ†¶é«”ä½¿ç”¨æ¸¬è©¦
        print(f"\n   è¨˜æ†¶é«”ä½¿ç”¨æ¸¬è©¦...")
        
        if torch.cuda.is_available():
            device = torch.device('cuda')
            model_gpu = model.to(device)
            
            test_input_gpu = {
                'price_frame': torch.randn(8, n_stocks, config.sequence_length, config.price_features).to(device),
                'fundamental': torch.randn(8, config.fundamental_features).to(device),
                'account': torch.randn(8, config.account_features).to(device)
            }
            
            torch.cuda.empty_cache()
            torch.cuda.reset_peak_memory_stats()
            
            with torch.no_grad():
                outputs = model_gpu(test_input_gpu)
            
            peak_memory = torch.cuda.max_memory_allocated() / 1024 / 1024  # MB
            print(f"     GPU å³°å€¼è¨˜æ†¶é«”ä½¿ç”¨: {peak_memory:.2f} MB")
            
            model = model.to('cpu')  # ç§»å› CPU
        else:
            print("     GPU ä¸å¯ç”¨ï¼Œè·³é GPU è¨˜æ†¶é«”æ¸¬è©¦")
        
        print("âœ… æ€§èƒ½åŸºæº–æ¸¬è©¦å®Œæˆ")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ€§èƒ½åŸºæº–æ¸¬è©¦å¤±æ•—: {str(e)}")
        traceback.print_exc()
        return False

def run_end_to_end_training_test():
    """åŸ·è¡Œç«¯åˆ°ç«¯è¨“ç·´æ¸¬è©¦"""
    print("=" * 60)
    print("TSE Alpha ç«¯åˆ°ç«¯è¨“ç·´æ¸¬è©¦")
    print("æ¸¬è©¦å®Œæ•´çš„è¨“ç·´æµç¨‹ï¼Œå¾è³‡æ–™è¼‰å…¥åˆ°æ¨¡å‹è¨“ç·´")
    print("=" * 60)
    
    tests_passed = 0
    total_tests = 6
    
    # åˆå§‹åŒ–è®Šæ•¸
    data_loader = None
    dataset = None
    config = None
    model = None
    model_config = None
    training_history = None
    
    # æ¸¬è©¦ 1: è³‡æ–™è¼‰å…¥å’Œé è™•ç†
    success, data_loader, dataset, config = test_data_loading()
    if success:
        tests_passed += 1
    
    # æ¸¬è©¦ 2: æ¨¡å‹è¨­ç½®å’Œåˆå§‹åŒ–
    if config:
        success, model, model_config = test_model_setup(config)
        if success:
            tests_passed += 1
    
    # æ¸¬è©¦ 3: è¨“ç·´å¾ªç’°
    if model and config:
        success, training_history = test_training_loop(model, config)
        if success:
            tests_passed += 1
    
    # æ¸¬è©¦ 4: æ¨¡å‹ä¿å­˜å’Œè¼‰å…¥
    if model and config:
        success = test_model_saving_loading(model, config)
        if success:
            tests_passed += 1
    
    # æ¸¬è©¦ 5: ç’°å¢ƒæ•´åˆæ¸¬è©¦
    if model and config:
        success = test_environment_integration(model, config)
        if success:
            tests_passed += 1
    
    # æ¸¬è©¦ 6: æ€§èƒ½åŸºæº–æ¸¬è©¦
    if model and config:
        success = test_performance_benchmark(model, config)
        if success:
            tests_passed += 1
    
    # æ¸¬è©¦çµæœç¸½çµ
    print("\n" + "=" * 60)
    print("ğŸ“‹ ç«¯åˆ°ç«¯è¨“ç·´æ¸¬è©¦çµæœ")
    print("=" * 60)
    
    pass_rate = (tests_passed / total_tests) * 100
    
    print(f"ç¸½æ¸¬è©¦æ•¸: {total_tests}")
    print(f"é€šéæ¸¬è©¦: {tests_passed}")
    print(f"å¤±æ•—æ¸¬è©¦: {total_tests - tests_passed}")
    print(f"é€šéç‡: {pass_rate:.1f}%")
    
    if pass_rate >= 85:
        print(f"\nğŸ‰ ç«¯åˆ°ç«¯è¨“ç·´æ¸¬è©¦é€šéï¼")
        print(f"âœ… å®Œæ•´è¨“ç·´æµç¨‹æ­£å¸¸é‹ä½œ")
        print(f"âœ… è³‡æ–™è¼‰å…¥å’Œæ¨¡å‹è¨“ç·´æˆåŠŸ")
        print(f"âœ… æ¨¡å‹ä¿å­˜è¼‰å…¥æ©Ÿåˆ¶æ­£å¸¸")
        print(f"âœ… ç’°å¢ƒæ•´åˆæ¸¬è©¦é€šé")
        print(f"ğŸš€ ç³»çµ±å·²æº–å‚™å¥½é€²è¡Œç”Ÿç”¢è¨“ç·´")
        
        print(f"\nğŸ¯ å»ºè­°ä¸‹ä¸€æ­¥:")
        print(f"   1. é€²è¡Œæ€§èƒ½åŸºæº–æ¸¬è©¦")
        print(f"   2. æ¸¬è©¦å›æ¸¬å¼•æ“")
        print(f"   3. é–‹å§‹å°è¦æ¨¡ç”Ÿç”¢è¨“ç·´")
        print(f"   4. æ“´å±•åˆ°å®Œæ•´180æ”¯è‚¡ç¥¨è¨“ç·´")
        
    elif pass_rate >= 70:
        print(f"\nâœ… ç«¯åˆ°ç«¯è¨“ç·´åŸºæœ¬å¯ç”¨")
        print(f"ğŸ”§ éƒ¨åˆ†åŠŸèƒ½å¯èƒ½éœ€è¦å„ªåŒ–")
        
    else:
        print(f"\nâš ï¸ ç«¯åˆ°ç«¯è¨“ç·´å­˜åœ¨é‡è¦å•é¡Œ")
        print(f"ğŸ”§ éœ€è¦ä¿®å¾©å¤±æ•—çš„æ¸¬è©¦é …ç›®")
    
    return pass_rate >= 70

if __name__ == "__main__":
    success = run_end_to_end_training_test()
    print(f"\n{'âœ… æ¸¬è©¦é€šé' if success else 'âŒ æ¸¬è©¦å¤±æ•—'}")
    sys.exit(0 if success else 1)